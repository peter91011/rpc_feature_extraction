{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from itertools import chain\n",
    "import os\n",
    "os.chdir('/home/y0c07y1/rpc_feature_extraction/rpc_model')\n",
    "import tqdm\n",
    "import glob\n",
    "import gcsfs\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob.glob('*.{}'.format('csv'))\n",
    "# files.sort()\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_directoty_individual = 'gs://msc_fair_airflow/rpc_model/sample_data_individual_feat/'\n",
    "gcp_directoty_full = 'gs://msc_fair_airflow/rpc_model/sample_data_full_feat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = extract_gcs_files(gcp_directoty_individual)\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2022-12-19', '2022-12-26', '2023-01-02', '2023-01-09', '2023-01-16'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_dates = set(i[-14:-4] for i in files)\n",
    "ref_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# start_date = '2022-12-19'\n",
    "# end_date = '2023-01-16'\n",
    "# ref_dates = pd.date_range(start=pd.to_datetime(start_date), end=pd.to_datetime(end_date), freq='7D')\n",
    "# ref_dates = [x.strftime('%Y-%m-%d') for x in ref_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on date 2023-01-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/mds_backfill/.local/lib/python3.6/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge catalog features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (1,3,4,5,6,7,8,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge site features\n",
      "merge site features\n",
      "merge bounce features\n",
      "merge bounce features\n",
      "working on date 2023-01-09\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge sem coop features\n",
      "merge catalog features\n",
      "merge site features\n",
      "merge site features\n"
     ]
    }
   ],
   "source": [
    "for date in ref_dates:\n",
    "    print('working on date '+date)\n",
    "    # read header\n",
    "    file = [x for x in files if x[-14:-4] == date and 'header' in x][0]\n",
    "    df_feat_all = pd.read_csv(file, index_col=0)\n",
    "    \n",
    "    df_feat_all = df_feat_all[(~pd.isnull(df_feat_all['catalog_item_id']))&(~pd.isnull(df_feat_all['seller_id']))]\n",
    "    df_feat_all['catalog_item_id'] = df_feat_all['catalog_item_id'].astype('int')\n",
    "    df_feat_all['catalog_item_id'] = df_feat_all['catalog_item_id'].astype('str')\n",
    "    df_feat_all['seller_id'] = df_feat_all['seller_id'].astype('int')\n",
    "    df_feat_all['seller_id'] = df_feat_all['seller_id'].astype('str')\n",
    "    df_feat_all.drop_duplicates(subset = 'adid', inplace=True)\n",
    "    \n",
    "    # drop off duplicates catalog_item_id, seller_id\n",
    "    \n",
    "#     # read targets\n",
    "#     file = 'sem_l_'+date+'.csv'\n",
    "#     df_tmp = pd.read_csv(file, index_col=0)\n",
    "#     df_feat_all = df_tmp.merge(df_feat_all, left_on = 'catalog_item_id', right_on = 'catalog_item_id', how='left')\n",
    "    \n",
    "    # read sem coop features\n",
    "    files_to_read = [x for x in files if x[-14:-4] == date and 'semcoop' in x]\n",
    "    \n",
    "    # left join to ensure item coverage is based on target coverage\n",
    "    for file in files_to_read:\n",
    "        print('merge sem coop features')\n",
    "        df_tmp = pd.read_csv(file, index_col=0)\n",
    "        df_tmp = df_tmp[(~pd.isnull(df_tmp['catalog_item_id']))&(~pd.isnull(df_tmp['seller_id']))]\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('int')\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('str')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('int')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('str')\n",
    "        df_tmp.drop_duplicates(subset = 'adid', inplace=True)\n",
    "        df_feat_all = df_feat_all.merge(df_tmp, on=['catalog_item_id', 'seller_id', 'source_id', 'adid', 'is_mobile'], how='left')\n",
    "        del df_tmp\n",
    "        \n",
    "    # read catalog feature\n",
    "    files_to_read = [x for x in files if x[-14:-4] == date and 'catalog' in x]\n",
    "    for file in files_to_read:\n",
    "        print('merge catalog features')\n",
    "        df_tmp = pd.read_csv(file, index_col=0)\n",
    "        df_tmp = df_tmp[(~pd.isnull(df_tmp['catalog_item_id']))&(~pd.isnull(df_tmp['seller_id']))]\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('int')\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('str')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('int')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('str')\n",
    "        df_tmp.drop_duplicates(subset = ['catalog_item_id','seller_id'], inplace=True)\n",
    "        df_feat_all = df_feat_all.merge(df_tmp, on=['catalog_item_id', 'seller_id'], how='left')\n",
    "        del df_tmp\n",
    "        \n",
    "    # read site features\n",
    "    files_to_read = [x for x in files if x[-14:-4] == date and 'site' in x]\n",
    "    for file in files_to_read:\n",
    "        print('merge site features')\n",
    "        df_tmp = pd.read_csv(file, index_col=0)\n",
    "        df_tmp = df_tmp[(~pd.isnull(df_tmp['catalog_item_id']))&(~pd.isnull(df_tmp['seller_id']))]\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('int')\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('str')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('int')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('str')\n",
    "        df_tmp.drop_duplicates(subset = ['catalog_item_id','seller_id'], inplace=True)\n",
    "        df_feat_all = df_feat_all.merge(df_tmp, on=['catalog_item_id', 'seller_id'], how='left')\n",
    "        del df_tmp\n",
    "        \n",
    "    # read bounce features\n",
    "    files_to_read = [x for x in files if x[-14:-4] == date and 'bounce' in x]\n",
    "    for file in files_to_read:\n",
    "        print('merge bounce features')\n",
    "        df_tmp = pd.read_csv(file, index_col=0)\n",
    "        df_tmp = df_tmp[(~pd.isnull(df_tmp['catalog_item_id']))&(~pd.isnull(df_tmp['seller_id']))]\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('int')\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('str')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('int')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('str')\n",
    "        df_tmp.drop_duplicates(subset = ['catalog_item_id','seller_id'], inplace=True)\n",
    "        df_feat_all = df_feat_all.merge(df_tmp, on=['catalog_item_id', 'seller_id'], how='left')\n",
    "        del df_tmp\n",
    "        \n",
    "#     df_feat_all.to_csv('../sample_data_full_feat/df_feat_all_coop_item_'+date+'.csv')\n",
    "    upload_csv_to_cloud_storage(df_feat_all, gcp_directoty_full+'df_feat_all_coop_item_'+date+'.csv')\n",
    "    del df_feat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on date 2022-11-20\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge catalog features\n",
      "merge site features\n",
      "merge site features\n",
      "merge bounce features\n",
      "merge bounce features\n",
      "working on date 2022-11-13\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge catalog features\n",
      "merge site features\n",
      "merge site features\n",
      "merge bounce features\n",
      "merge bounce features\n",
      "working on date 2022-11-06\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge catalog features\n",
      "merge site features\n",
      "merge site features\n",
      "merge bounce features\n",
      "merge bounce features\n",
      "working on date 2022-10-30\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge sem features\n",
      "merge catalog features\n",
      "merge site features\n",
      "merge site features\n",
      "merge bounce features\n",
      "merge bounce features\n"
     ]
    }
   ],
   "source": [
    "for date in ref_dates[2:][::-1]:\n",
    "    print('working on date '+date)\n",
    "    # read header\n",
    "    file = 'df_header_'+date+'.csv'\n",
    "    df_feat_all = pd.read_csv(file, index_col=0)\n",
    "    \n",
    "    df_feat_all = df_feat_all[(~pd.isnull(df_feat_all['catalog_item_id']))&(~pd.isnull(df_feat_all['seller_id']))]\n",
    "    df_feat_all['catalog_item_id'] = df_feat_all['catalog_item_id'].astype('int')\n",
    "    df_feat_all['catalog_item_id'] = df_feat_all['catalog_item_id'].astype('str')\n",
    "    df_feat_all['seller_id'] = df_feat_all['seller_id'].astype('int')\n",
    "    df_feat_all['seller_id'] = df_feat_all['seller_id'].astype('str')\n",
    "    df_feat_all.drop_duplicates(subset = 'adid', inplace=True)\n",
    "    \n",
    "    # drop off duplicates catalog_item_id, seller_id\n",
    "    \n",
    "#     # read targets\n",
    "#     file = 'sem_l_'+date+'.csv'\n",
    "#     df_tmp = pd.read_csv(file, index_col=0)\n",
    "#     df_feat_all = df_tmp.merge(df_feat_all, left_on = 'catalog_item_id', right_on = 'catalog_item_id', how='left')\n",
    "    \n",
    "    # read sem features\n",
    "    files_to_read = [x for x in files if x[-14:-4] == date and x[:4]=='sem_']\n",
    "    \n",
    "    # left join to ensure item coverage is based on target coverage\n",
    "    for file in files_to_read:\n",
    "        print('merge sem features')\n",
    "        df_tmp = pd.read_csv(file, index_col=0)\n",
    "        df_tmp = df_tmp[(~pd.isnull(df_tmp['catalog_item_id']))&(~pd.isnull(df_tmp['seller_id']))]\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('int')\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('str')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('int')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('str')\n",
    "        df_tmp.drop_duplicates(subset = 'adid', inplace=True)\n",
    "        df_feat_all = df_feat_all.merge(df_tmp, on=['catalog_item_id', 'seller_id', 'source_id', 'adid', 'is_mobile'], how='left')\n",
    "        del df_tmp\n",
    "        \n",
    "    # read catalog feature\n",
    "    files_to_read = [x for x in files if x[-14:-4] == date and x[:7]=='catalog']\n",
    "    for file in files_to_read:\n",
    "        print('merge catalog features')\n",
    "        df_tmp = pd.read_csv(file, index_col=0)\n",
    "        df_tmp = df_tmp[(~pd.isnull(df_tmp['catalog_item_id']))&(~pd.isnull(df_tmp['seller_id']))]\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('int')\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('str')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('int')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('str')\n",
    "        df_tmp.drop_duplicates(subset = ['catalog_item_id','seller_id'], inplace=True)\n",
    "        df_feat_all = df_feat_all.merge(df_tmp, on=['catalog_item_id', 'seller_id'], how='left')\n",
    "        del df_tmp\n",
    "        \n",
    "    # read site features\n",
    "    files_to_read = [x for x in files if x[-14:-4] == date and x[:4]=='site' and 'item' in x]\n",
    "    for file in files_to_read:\n",
    "        print('merge site features')\n",
    "        df_tmp = pd.read_csv(file, index_col=0)\n",
    "        df_tmp = df_tmp[(~pd.isnull(df_tmp['catalog_item_id']))&(~pd.isnull(df_tmp['seller_id']))]\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('int')\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('str')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('int')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('str')\n",
    "        df_tmp.drop_duplicates(subset = ['catalog_item_id','seller_id'], inplace=True)\n",
    "        df_feat_all = df_feat_all.merge(df_tmp, on=['catalog_item_id', 'seller_id'], how='left')\n",
    "        del df_tmp\n",
    "        \n",
    "    # read bounce features\n",
    "    files_to_read = [x for x in files if x[-14:-4] == date and x[:6]=='bounce' and 'item' in x]\n",
    "    for file in files_to_read:\n",
    "        print('merge bounce features')\n",
    "        df_tmp = pd.read_csv(file, index_col=0)\n",
    "        df_tmp = df_tmp[(~pd.isnull(df_tmp['catalog_item_id']))&(~pd.isnull(df_tmp['seller_id']))]\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('int')\n",
    "        df_tmp['catalog_item_id'] = df_tmp['catalog_item_id'].astype('str')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('int')\n",
    "        df_tmp['seller_id'] = df_tmp['seller_id'].astype('str')\n",
    "        df_tmp.drop_duplicates(subset = ['catalog_item_id','seller_id'], inplace=True)\n",
    "        df_feat_all = df_feat_all.merge(df_tmp, on=['catalog_item_id', 'seller_id'], how='left')\n",
    "        del df_tmp\n",
    "        \n",
    "    df_feat_all.to_csv('../sample_data_full_feat/df_feat_all_item_'+date+'.csv')\n",
    "    del df_feat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
