{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql.functions import broadcast\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "os.chdir('/home/y0c07y1/rpc_feature_extraction/rpc_model/')\n",
    "from utils.utils import *\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import functions as func\n",
    "import pyspark.sql as pss\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    MapType,\n",
    "    LongType,\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    IntegerType,\n",
    "    ArrayType\n",
    ")\n",
    "#spark = pss.SparkSession.builder.appName(\"dctr_test3\").config(conf=pyspark.SparkConf().setAll([('spark.yarn.queue', 'root.critical')])).enableHiveSupport().getOrCreate()\n",
    "def gen_spark_session(ds):\n",
    "    ds_nodash = ds.replace(\"-\", \"\")\n",
    "\n",
    "    return pyspark.sql.SparkSession.builder.config(\n",
    "            \"hive.exec.dynamic.partition\", True,\n",
    "        ).config(\n",
    "            \"hive.exec.dynamic.partition.mode\", \"nonstrict\"\n",
    "        ).config('mapreduce.input.fileinputformat.input.dir.recursive',True\n",
    "        ).config('spark.hive.mapred.supports.subdirectories',True\n",
    "        ).config(\"spark.sql.crossJoin.enabled\", \"true\"\n",
    "        ).config(\"spark.sql.broadcastTimeout\", \"36000\"\n",
    "        ).config(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\"\n",
    "        ).config(\"spark.hadoop.orc.overwrite.output.file\",True\n",
    "        ).config(\"spark.yarn.executor.memoryOverhead\", '20g'\n",
    "        ).config(\"spark.executor.memory\",'12g'\n",
    "        ).config(\"spark.driver.memory\",'12g'\n",
    "        ).config(\"spark.dynamicAllocation.enabled\",'true'\n",
    "        ).config(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",'true'\n",
    "        ).config(\"spark.sql.hive.convertMetastoreOrc\", 'false'\n",
    "        ).appName(APP_NAME % ds_nodash).enableHiveSupport().getOrCreate()\n",
    "\n",
    "\n",
    "APP_NAME = 'rpc_feature_extractionjupyter1_%s'\n",
    "spark = gen_spark_session('2022-12/7')\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define functions to get sem, catalog, and site signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## func to join catalog_item_id to dctr pla desktop ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dctr_item_id(today_ds):\n",
    "    \n",
    "    # this function is to retrieve the item sets that has traffic information in sem.daily_criteria_traffic_revenue in the time interval of\n",
    "    # (today_ds - 139 days, today_ds - 7 days)\n",
    "    # the features are extracted from (today_ds - 139 days, today_ds - 7 days), and the target values will be constructed based on (today_ds - 6days, today)\n",
    "    # basicially predicting \"NEXT WEEK\" values (conversion rate, ordersize)\n",
    "    # start from 2022-7-26 since data is only available after 0726 for v2 pla data\n",
    "    partitionDate = spark.sql(\"\"\"show partitions dmas.dfm\"\"\").rdd.flatMap(lambda x:x).map(lambda x : x.replace(\"ts=\",\"\")).max()\n",
    "    \n",
    "    query = spark.sql(\"\"\"SELECT dctr.*,\n",
    "                                coalesce(pcoc.catalog_item_id, catalog_item_id_dctr) as catalog_item_id,\n",
    "                                coalesce(pcoc.us_seller_id, seller_id_dctr) as seller_id,\n",
    "                                dmas.ad_item_brand brand_nm,\n",
    "                                darm.division_id AS division_id,\n",
    "                                darm.super_department_id AS super_department_id,\n",
    "                                darm.department_id AS department_id,\n",
    "                                darm.category_id AS category_id,\n",
    "                                darm.sub_category_id AS sub_category_id,\n",
    "                                item_dedup.id_unique_variant\n",
    "\n",
    "                        FROM (SELECT DISTINCT adid, source_id, is_mobile, seller_id as seller_id_dctr, split(adid,'_')[0] as catalog_item_id_dctr\n",
    "                              FROM sem.daily_criteria_traffic_revenue_v2_tmp\n",
    "                              WHERE \n",
    "                                   customer_id_p = '1' \n",
    "                                   AND (source_id = 2 or source_id=4)\n",
    "                                   AND adtype_p ='pla' \n",
    "                                   AND structure_version_p = 2\n",
    "                                   AND revenue_source_p = 'fair15'\n",
    "                                   AND date_string_p >= date_sub('__today__', 139) \n",
    "                                   AND date_string_p <= date_sub('__today__', 7)) dctr\n",
    "\n",
    "                              LEFT JOIN sem.pla_catalog_offer_criteria_shopping pcoc\n",
    "                              ON dctr.adid = pcoc.adid\n",
    "\n",
    "\n",
    "                              LEFT JOIN(\n",
    "                                        SELECT adid, last(division_id) as division_id,\n",
    "                                        last(sub_category_id) as sub_category_id, last(super_department_id) as super_department_id,\n",
    "                                        last(department_id) as department_id, last(category_id) as category_id\n",
    "                                        FROM sem.daily_adid_rh_metrics_v2 \n",
    "                                        WHERE customer_id_p=1\n",
    "                                        AND structure_version_p = 2\n",
    "                                        AND revenue_source_p = 'fair15'\n",
    "                                        AND date_string >= date_sub('__today__', 139) \n",
    "                                        AND date_string <= date_sub('__today__', 7)\n",
    "                                        GROUP BY adid\n",
    "                                        ) darm\n",
    "                            ON dctr.adid = darm.adid \n",
    "\n",
    "                              LEFT JOIN (SELECT item_id as catlg_item_id,\n",
    "                                        max(lower(REGEXP_REPLACE(item_brand_name, '[^0-9A-Za-z]', ''))) as ad_item_brand\n",
    "                                        FROM dmas.dfm\n",
    "                                        WHERE ts='__partitionDate__'\n",
    "                                        GROUP BY item_id) dmas\n",
    "                              ON coalesce(pcoc.catalog_item_id, dctr.catalog_item_id_dctr) = dmas.catlg_item_id\n",
    "                              \n",
    "                              LEFT JOIN (SELECT item_id,id_unique_variant \n",
    "                                          FROM casesci_sem.items_w_dups_mapping_sem_offer_bidding\n",
    "                                          WHERE date_string='latest') item_dedup\n",
    "                              ON coalesce(pcoc.catalog_item_id, dctr.catalog_item_id_dctr) = item_dedup.item_id\n",
    "                              \"\"\".replace('__today__', today_ds).replace('__partitionDate__', partitionDate))\n",
    "    \n",
    "    query.createOrReplaceTempView(\"dctr_pla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## func for sem signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semSignalCoop(today_ds, days_backtrack_end, days_backtrack_start, bad_days):\n",
    "    \n",
    "    # this function is to retrieve the \"SEM\" signals of each item/ad\n",
    "    # SEM signals means the performance of an item/ad in SEM channel (Search Engine Marketing)\n",
    "    # signals are extracted from different time intervals as specified in the json file - 'output_sem_performance_prediction_config_train_config_20201227.json'\n",
    "    \n",
    "    query = \"\"\"WITH             \n",
    "        dctr\n",
    "        AS\n",
    "        (SELECT dctr_v2.adid,\n",
    "                dctr_v2.source_id,\n",
    "                dctr_v2.is_mobile,\n",
    "                SUM(impressions) AS impressions,\n",
    "                SUM(clicks) AS clicks,\n",
    "                SUM(adspend) AS adspend,\n",
    "                SUM(qty) AS qty,\n",
    "                MAX(att_lineitems_brand) AS orders_brand,\n",
    "                MAX(att_lineitems_seller) AS orders_seller,\n",
    "                MAX(attributed_gmv_brand) AS gmv_revenue_adjustment_brand,\n",
    "                MAX(attributed_gmv_seller) AS gmv_revenue_adjustment_seller,\n",
    "                -- SUM(orders) AS orders,\n",
    "                -- SUM(gmv_revenue_adjustment) AS gmv_revenue_adjustment,\n",
    "                SUM(cost) AS cost,\n",
    "                SUM(estimated_cp) AS estimated_cp\n",
    "\n",
    "        FROM (SELECT * FROM sem.daily_criteria_traffic_revenue_v2_tmp\n",
    "               WHERE customer_id_p = '1' \n",
    "               AND (source_id = 2 or source_id=4)\n",
    "               AND adtype_p ='pla' \n",
    "               AND structure_version_p = 2\n",
    "               AND revenue_source_p = 'fair15'\n",
    "               AND date_string_p >= date_sub('__today__', __daysback_end__) \n",
    "               AND date_string_p <= date_sub('__today__', __daysback_start__) \n",
    "               AND date_string_p NOT IN __bad_days__) dctr_v2\n",
    "        LEFT JOIN (\n",
    "        SELECT adid, is_mobile, source_id, SUM(att_lineitems_brand) as att_lineitems_brand, SUM(att_lineitems_seller) as att_lineitems_seller,\n",
    "                SUM(attributed_gmv_brand) as attributed_gmv_brand, SUM(attributed_gmv_seller) as attributed_gmv_seller\n",
    "        FROM fair_dev.pt3_14day_report_nas_test \n",
    "        WHERE\n",
    "                ad_date >= date_sub('__today__', __daysback_end__) \n",
    "                AND ad_date <= date_sub('__today__', __daysback_start__) \n",
    "                AND ad_date NOT IN __bad_days__ \n",
    "                AND adid != 'NULL'\n",
    "                AND is_pla = 1\n",
    "        GROUP BY adid, is_mobile, source_id\n",
    "        ) coop_14d\n",
    "        ON dctr_v2.adid = coop_14d.adid\n",
    "        AND dctr_v2.is_mobile = coop_14d.is_mobile\n",
    "        AND dctr_v2.source_id = coop_14d.source_id\n",
    "\n",
    "        GROUP BY dctr_v2.adid, dctr_v2.is_mobile, dctr_v2.source_id)\n",
    "\n",
    "           --final query\n",
    "           SELECT  dctr_pla.catalog_item_id AS catalog_item_id,\n",
    "                   coalesce(dctr_pla.id_unique_variant, dctr_pla.catalog_item_id) AS id_unique_variant,\n",
    "                   dctr_pla.seller_id AS seller_id,\n",
    "                   dctr_pla.source_id AS source_id,\n",
    "                   dctr_pla.adid AS adid,\n",
    "                   dctr_pla.brand_nm AS brand_nm,\n",
    "                   dctr_pla.division_id AS division_id,\n",
    "                   dctr_pla.super_department_id AS super_department_id,\n",
    "                   dctr_pla.department_id AS department_id,\n",
    "                   dctr_pla.category_id AS category_id,\n",
    "                   dctr_pla.sub_category_id AS sub_category_id,\n",
    "                   dctr_pla.is_mobile AS is_mobile,\n",
    "                   dctr.impressions/__tot_days__ AS impressions,\n",
    "                   dctr.clicks/__tot_days__ AS clicks,\n",
    "                   dctr.adspend/__tot_days__ AS adspend,\n",
    "                   dctr.qty/__tot_days__ AS qty,\n",
    "                   dctr.orders_brand/__tot_days__ AS orders_brand,\n",
    "                   dctr.orders_seller/__tot_days__ AS orders_seller,\n",
    "                   dctr.gmv_revenue_adjustment_seller/__tot_days__ AS gmv_revenue_adjustment_seller,\n",
    "                   dctr.gmv_revenue_adjustment_brand/__tot_days__ AS gmv_revenue_adjustment_brand,\n",
    "                   dctr.cost/__tot_days__ AS cost,\n",
    "                   (dctr.gmv_revenue_adjustment_seller - dctr.cost)/__tot_days__ AS margin_seller,\n",
    "                   (dctr.gmv_revenue_adjustment_brand - dctr.cost)/__tot_days__ AS margin_brand,\n",
    "                   dctr.estimated_cp/__tot_days__ AS estimated_cp,\n",
    "                   dctr.clicks/dctr.impressions AS ctr,\n",
    "                   dctr.adspend/dctr.clicks AS cpc,\n",
    "                   dctr.gmv_revenue_adjustment_brand/dctr.clicks AS rpc_brand,\n",
    "                   dctr.gmv_revenue_adjustment_seller/dctr.clicks AS rpc_seller,\n",
    "                   dctr.gmv_revenue_adjustment_brand/dctr.adspend AS roas_brand,\n",
    "                   dctr.gmv_revenue_adjustment_seller/dctr.adspend AS roas_seller,\n",
    "                   dctr.orders_brand/dctr.clicks AS convrt_brand,\n",
    "                   dctr.orders_seller/dctr.clicks AS convrt_seller,\n",
    "                   dctr.gmv_revenue_adjustment_brand/dctr.orders_brand AS ordersize_brand,\n",
    "                   dctr.gmv_revenue_adjustment_seller/dctr.orders_seller AS ordersize_seller,\n",
    "                   dctr.estimated_cp/dctr.orders_brand AS cpsize_brand,\n",
    "                   dctr.estimated_cp/dctr.orders_seller AS cpsize_seller,\n",
    "                   (dctr.gmv_revenue_adjustment_brand - dctr.cost)/dctr.clicks AS mpc_brand,\n",
    "                   (dctr.gmv_revenue_adjustment_seller - dctr.cost)/dctr.clicks AS mpc_seller,\n",
    "                   (dctr.gmv_revenue_adjustment_brand - dctr.cost)/dctr.orders_brand AS mpo_brand,\n",
    "                   (dctr.gmv_revenue_adjustment_seller - dctr.cost)/dctr.orders_seller AS mpo_seller,\n",
    "                   dctr.gmv_revenue_adjustment_brand/dctr.estimated_cp AS rpcp_brand,\n",
    "                   dctr.gmv_revenue_adjustment_seller/dctr.estimated_cp AS rpcp_seller\n",
    "\n",
    "            FROM dctr_pla LEFT JOIN dctr\n",
    "                 ON dctr_pla.adid = dctr.adid\n",
    "                 AND dctr_pla.source_id = dctr.source_id\n",
    "                 AND dctr_pla.is_mobile = dctr.is_mobile\n",
    "            \"\"\"\n",
    "    \n",
    "    query = query.replace('__today__', today_ds)\n",
    "    query = query.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    query = query.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    query = query.replace('__bad_days__', bad_days)\n",
    "    bad_days_list = [i.replace(\"('\",'').replace(\"')\",'') for i in bad_days.split(\"', '\")]\n",
    "    exclude_days = len([datetime.strptime(dat, '%Y-%m-%d') for dat in bad_days_list if datetime.strptime(dat, '%Y-%m-%d').date()>=datetime.strptime(today_ds, '%Y-%m-%d').date()- \\\n",
    "                 timedelta(days=days_backtrack_end) and datetime.strptime(dat, '%Y-%m-%d').date()<=datetime.strptime(today_ds, '%Y-%m-%d').date()- timedelta(days=days_backtrack_start)])\n",
    "    query = query.replace('__tot_days__', str(days_backtrack_end - days_backtrack_start+1-exclude_days))\n",
    "    \n",
    "    df = spark.sql(query).toPandas()\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semSignal(today_ds, days_backtrack_end, days_backtrack_start, bad_days):\n",
    "    \n",
    "    # this function is to retrieve the \"SEM\" signals of each item/ad\n",
    "    # SEM signals means the performance of an item/ad in SEM channel (Search Engine Marketing)\n",
    "    # signals are extracted from different time intervals as specified in the json file - 'output_sem_performance_prediction_config_train_config_20201227.json'\n",
    "    \n",
    "    query = \"\"\"WITH             \n",
    "            dctr\n",
    "            AS\n",
    "            (SELECT adid,\n",
    "                    source_id,\n",
    "                    is_mobile,\n",
    "                    SUM(impressions) AS impressions,\n",
    "                    SUM(clicks) AS clicks,\n",
    "                    SUM(adspend) AS adspend,\n",
    "                    SUM(qty) AS qty,\n",
    "                    SUM(orders) AS orders,\n",
    "                    SUM(gmv_revenue_adjustment) AS gmv_revenue_adjustment,\n",
    "                    SUM(cost) AS cost,\n",
    "                    SUM(estimated_cp) AS estimated_cp\n",
    "                              \n",
    "            FROM sem.daily_criteria_traffic_revenue_v2_tmp\n",
    "            WHERE\n",
    "                    customer_id_p = '1' \n",
    "                   AND (source_id = 2 or source_id=4)\n",
    "                   AND adtype_p ='pla' \n",
    "                   AND structure_version_p = 2\n",
    "                   AND revenue_source_p = 'fair15'\n",
    "                   AND date_string_p >= date_sub('__today__', __daysback_end__) \n",
    "                   AND date_string_p <= date_sub('__today__', __daysback_start__) \n",
    "                   AND date_string_p NOT IN __bad_days__\n",
    "            GROUP BY adid, is_mobile, source_id)\n",
    "            \n",
    "               --final query\n",
    "               SELECT  dctr_pla.catalog_item_id AS catalog_item_id,\n",
    "                       dctr_pla.seller_id AS seller_id,\n",
    "                       dctr_pla.adid AS adid,\n",
    "                       dctr_pla.brand_nm AS brand_nm,\n",
    "                       dctr_pla.division_id AS division_id,\n",
    "                       dctr_pla.super_department_id AS super_department_id,\n",
    "                       dctr_pla.department_id AS department_id,\n",
    "                       dctr_pla.category_id AS category_id,\n",
    "                       dctr_pla.sub_category_id AS sub_category_id,\n",
    "                       dctr_pla.source_id AS source_id,\n",
    "                       dctr_pla.is_mobile AS is_mobile,\n",
    "                       dctr.impressions/__tot_days__ AS impressions,\n",
    "                       dctr.clicks/__tot_days__ AS clicks,\n",
    "                       dctr.adspend/__tot_days__ AS adspend,\n",
    "                       dctr.qty/__tot_days__ AS qty,\n",
    "                       dctr.orders/__tot_days__ AS orders,\n",
    "                       dctr.gmv_revenue_adjustment/__tot_days__ AS gmv_revenue_adjustment,\n",
    "                       dctr.cost/__tot_days__ AS cost,\n",
    "                       (dctr.gmv_revenue_adjustment - dctr.cost)/__tot_days__ AS margin,\n",
    "                       dctr.estimated_cp/__tot_days__ AS estimated_cp,\n",
    "                       dctr.clicks/dctr.impressions AS ctr,\n",
    "                       dctr.adspend/dctr.clicks AS cpc,\n",
    "                       dctr.gmv_revenue_adjustment/dctr.clicks AS rpc,\n",
    "                       dctr.gmv_revenue_adjustment/dctr.adspend AS roas,\n",
    "                       dctr.orders/dctr.clicks AS convrt,\n",
    "                       dctr.gmv_revenue_adjustment/dctr.orders AS ordersize,\n",
    "                       dctr.estimated_cp/dctr.orders AS cpsize,\n",
    "                       (dctr.gmv_revenue_adjustment - dctr.cost)/dctr.clicks AS mpc,\n",
    "                       (dctr.gmv_revenue_adjustment - dctr.cost)/dctr.orders AS mpo,\n",
    "                       dctr.gmv_revenue_adjustment/dctr.estimated_cp AS rpcp\n",
    "                       \n",
    "                FROM dctr_pla LEFT JOIN dctr\n",
    "                     ON dctr_pla.adid = dctr.adid\n",
    "                     AND dctr_pla.source_id = dctr.source_id\n",
    "                     AND dctr_pla.is_mobile = dctr.is_mobile\n",
    "                \"\"\"\n",
    "    query = query.replace('__today__', today_ds)\n",
    "    query = query.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    query = query.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    query = query.replace('__bad_days__', bad_days)\n",
    "    bad_days_list = [i.replace(\"('\",'').replace(\"')\",'') for i in bad_days.split(\"', '\")]\n",
    "    exclude_days = len([datetime.strptime(dat, '%Y-%m-%d') for dat in bad_days_list if datetime.strptime(dat, '%Y-%m-%d').date()>=datetime.strptime(today_ds, '%Y-%m-%d').date()- \\\n",
    "                 timedelta(days=days_backtrack_end) and datetime.strptime(dat, '%Y-%m-%d').date()<=datetime.strptime(today_ds, '%Y-%m-%d').date()- timedelta(days=days_backtrack_start)])\n",
    "    query = query.replace('__tot_days__', str(days_backtrack_end - days_backtrack_start+1-exclude_days))\n",
    "    \n",
    "    df = spark.sql(query).toPandas()\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## func for catalog signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catalogSignal(today_ds, days_backtrack_end, days_backtrack_start, bad_days):\n",
    "    \n",
    "    # this function is to retrieve the \"catalog\" signals of each item/ad\n",
    "    # catalog signals are the properties of an item recorded in the catalog system, like what category, department, etc of this item belongs to; also the price/\n",
    "    # price changes of an item\n",
    "    # signals are extracted from different time intervals as specified in the json file - 'output_sem_performance_prediction_config_train_config_20201227.json'\n",
    "\n",
    "    query = \"\"\"\n",
    "            WITH    \n",
    "            -- latest price\n",
    "            tab_latest_price\n",
    "            AS\n",
    "            (SELECT \n",
    "                 catalog_item_id,\n",
    "                 seller_id,\n",
    "                 max(seller_name) as seller_name,\n",
    "                 max(regexp_replace(division, \"\\t\", \"\")) AS division,\n",
    "                 max(regexp_replace(super_dept, \"\\t\", \"\")) AS super_dept,\n",
    "                 max(regexp_replace(dept, \"\\t\", \"\")) AS dept,\n",
    "                 max(regexp_replace(cat, \"\\t\", \"\")) AS cat,\n",
    "                 max(regexp_replace(subcat, \"\\t\", \"\")) AS subcat,\n",
    "                 max(brand_name) as brand_name,\n",
    "                 max(curr_item_price) as curr_item_price,\n",
    "                 max(price_change_desc) as price_change_desc,\n",
    "                 max(num_appr_reviews) as num_appr_reviews,\n",
    "                 max(avg_overall_rating) as avg_overall_rating,\n",
    "                 max(avg_cost) as avg_cost\n",
    "       \n",
    "               FROM casesci_cmn.uber_item_info_dfm_accumulate\n",
    "       \n",
    "               WHERE ds = date_sub('__today__', 14)\n",
    "               GROUP BY catalog_item_id, seller_id),\n",
    "               \n",
    "            -- average price\n",
    "            tab_avg_price\n",
    "            AS\n",
    "            (SELECT catalog_item_id, seller_id,\n",
    "                    AVG(curr_item_price) AS avg_item_price\n",
    "                    FROM casesci_cmn.uber_item_info_dfm_accumulate\n",
    "        \n",
    "                    WHERE ds >= date_sub('__today__', __daysback_end__)\n",
    "                      AND ds <= date_sub('__today__', __daysback_start__)\n",
    "                      AND ds NOT IN __bad_days__\n",
    "              \n",
    "                GROUP BY catalog_item_id, seller_id)\n",
    "                \n",
    "            -- final query\n",
    "            \n",
    "            SELECT dctr_pla_distinct.catalog_item_id AS catalog_item_id,\n",
    "                   dctr_pla_distinct.seller_id AS seller_id,\n",
    "                   tab_latest_price.seller_name,\n",
    "                   tab_latest_price.division,\n",
    "                   tab_latest_price.super_dept,\n",
    "                   tab_latest_price.dept,\n",
    "                   tab_latest_price.cat,\n",
    "                   tab_latest_price.subcat,\n",
    "                   tab_latest_price.brand_name,\n",
    "                   tab_latest_price.curr_item_price,\n",
    "                   tab_latest_price.price_change_desc,\n",
    "                   tab_latest_price.num_appr_reviews,\n",
    "                   tab_latest_price.avg_overall_rating,\n",
    "                   tab_latest_price.avg_cost,\n",
    "                   tab_latest_price.curr_item_price/tab_avg_price.avg_item_price AS price_change_ratio,\n",
    "                   tab_latest_price.curr_item_price - tab_avg_price.avg_item_price AS price_change_value,\n",
    "                   tab_latest_price.curr_item_price - tab_latest_price.avg_cost AS avg_margin\n",
    "                   \n",
    "            FROM (SELECT DISTINCT catalog_item_id, seller_id from dctr_pla) dctr_pla_distinct \n",
    "            LEFT JOIN tab_latest_price\n",
    "                 ON dctr_pla_distinct.catalog_item_id = tab_latest_price.catalog_item_id\n",
    "                 AND dctr_pla_distinct.seller_id = tab_latest_price.seller_id\n",
    "                 LEFT JOIN tab_avg_price\n",
    "                 ON tab_latest_price.catalog_item_id = tab_avg_price.catalog_item_id\n",
    "                 AND tab_latest_price.seller_id = tab_avg_price.seller_id\n",
    "     \"\"\"  \n",
    "    query = query.replace('__today__', today_ds)\n",
    "    query = query.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    query = query.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    query = query.replace('__bad_days__', bad_days)\n",
    "    sql = spark.sql(query)\n",
    "    df = sql.toPandas()\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## func for site signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siteSignal(today_ds, days_backtrack_end, days_backtrack_start, bad_days):\n",
    "    \n",
    "    # this function is to retrieve the \"site\" signals of each item/ad\n",
    "    # site signals are to measure the performance of an item/ad on whole Walmart site (SEM signals only indicate the performance of \"SEM\" channel)\n",
    "    # signals are extracted from different time intervals as specified in the json file - 'output_sem_performance_prediction_config_train_config_20201227.json'\n",
    "    \n",
    "    partitionDate = spark.sql(\"\"\"show partitions dmas.dfm\"\"\").rdd.flatMap(lambda x: x).map(lambda x: x.replace(\"ts=\", \"\")).max()\n",
    "    \n",
    "    l2 = \"\"\"SELECT session_id, dt, catlg_itm_id_set[0] as catalog_item_id, action_categ, action_sub_categ\n",
    "                FROM us_dl_interactions_restrict.interactions_l2_session_events\n",
    "                    WHERE vtc IS NOT NULL\n",
    "                          AND vtc != ''\n",
    "                          AND vtc != '\\\\N'\n",
    "                          AND catlg_itm_id_set[0] IS NOT NULL\n",
    "                          AND catlg_itm_id_set[0] != ''\n",
    "                          AND (action_categ = 'pageView' or action_sub_categ = 'addToCart')\n",
    "                          AND dt >= date_sub('__today__', __daysback_end__)\n",
    "                          AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                          AND dt NOT IN __bad_days__\n",
    "                          AND pipeline in ('PULSE_USOA_RWEB', 'ANIVIA_USOA_APP') \n",
    "                          AND context = 'productPage'\"\"\"\n",
    "    l2 = l2.replace('__today__', today_ds)\n",
    "    l2 = l2.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    l2 = l2.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    l2 = l2.replace('__bad_days__', bad_days)\n",
    "    l2 = l2.replace('__partitionDate__', partitionDate)\n",
    "    # session_id, dt, catalog_item_id, action_categ\n",
    "    spark.sql(l2).write.mode(\"overwrite\").partitionBy('dt', 'action_categ', 'action_sub_categ').saveAsTable('fair_dev.yp_test_interactions_l2')\n",
    "    \n",
    "    print('l2 saved for site signals')\n",
    "    \n",
    "    l1 = \"\"\"SELECT distinct session_id, dt, itm_list[0]['catlgItm']['sellerId'] as sellerid, action_categ, action_sub_categ,\n",
    "                            itm_list[0]['catlgItm']['productId'] as productid\n",
    "                            FROM us_dl_interactions_restrict.interactions_l1_cbb_catalog\n",
    "                            WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                            AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                            AND dt NOT IN __bad_days__\n",
    "                            AND itm_list is not null\n",
    "                            AND itm_list[0]['catlgItm']['sellerId'] IS NOT NULL\n",
    "                            AND itm_list[0]['catlgItm']['productId'] IS NOT NULL\n",
    "                            AND (action_categ = 'pageView' or action_sub_categ = 'addToCart')\n",
    "                            AND pipeline in ('PULSE_USOA_RWEB', 'ANIVIA_USOA_APP') \n",
    "                            AND context = 'productPage'\"\"\"\n",
    "    l1 = l1.replace('__today__', today_ds)\n",
    "    l1 = l1.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    l1 = l1.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    l1 = l1.replace('__bad_days__', bad_days)\n",
    "    l1 = l1.replace('__partitionDate__', partitionDate)\n",
    "    # session_id, dt, action_categ, sellerid, productid\n",
    "    spark.sql(l1).write.mode(\"overwrite\").partitionBy('dt', 'action_categ', 'action_sub_categ').saveAsTable('fair_dev.yp_test_interactions_l1')\n",
    "    \n",
    "    print('l1 saved for site signals')\n",
    "    \n",
    "    dmas_seller = spark.sql(\"select * from fair_dev.yp_test_dmas_seller\")\n",
    "    dmas_item = spark.sql(\"select * from fair_dev.yp_test_dmas_item\")\n",
    "    l1 = \"\"\"select * from fair_dev.yp_test_interactions_l1 \n",
    "            WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                            AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                            AND dt NOT IN __bad_days__\"\"\"\n",
    "    l1 = l1.replace('__today__', today_ds)\n",
    "    l1 = l1.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    l1 = l1.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    l1 = l1.replace('__bad_days__', bad_days)\n",
    "    l1_df = spark.sql(l1)\n",
    "    \n",
    "    l1_dmas = l1_df.join(broadcast(dmas_seller),dmas_seller.seller_id ==  l1_df.sellerid, how='inner').drop(l1_df.sellerid).drop(dmas_seller.seller_id)\n",
    "    l1_dmas = l1_dmas.join(dmas_item,dmas_item.product_id ==  l1_dmas.productid, how='left').drop(dmas_item.product_id).drop(l1_dmas.productid)\n",
    "    # session_id, dt, action_categ, us_seller_id, item_id on table l1\n",
    "    l1_dmas.write.mode(\"overwrite\").partitionBy('dt', 'action_categ', 'action_sub_categ').saveAsTable('fair_dev.yp_test_interactions_l1_dmas')\n",
    "    \n",
    "    print('pageview l1_dmas saved for site signals after getting seller_id and catalog_item_id from dmas table')\n",
    "    \n",
    "    l2 = \"\"\"select * from fair_dev.yp_test_interactions_l2 \n",
    "        WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                        AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                        AND dt NOT IN __bad_days__\"\"\"\n",
    "    l2 = l2.replace('__today__', today_ds)\n",
    "    l2 = l2.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    l2 = l2.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    l2 = l2.replace('__bad_days__', bad_days)\n",
    "    l2_df = spark.sql(l2)\n",
    "    \n",
    "    l1_dmas = \"\"\"select * from fair_dev.yp_test_interactions_l1_dmas \n",
    "        WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                        AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                        AND dt NOT IN __bad_days__\"\"\"\n",
    "    l1_dmas = l1_dmas.replace('__today__', today_ds)\n",
    "    l1_dmas = l1_dmas.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    l1_dmas = l1_dmas.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    l1_dmas = l1_dmas.replace('__bad_days__', bad_days)\n",
    "    l1_dmas_df = spark.sql(l1_dmas)\n",
    "    \n",
    "    l2_l1_dmas = l2_df.join(l1_dmas_df, [(l2_df.session_id == l1_dmas_df.session_id) & (l2_df.dt == l1_dmas_df.dt) & (l2_df.action_categ == l1_dmas_df.action_categ)], how = 'left')\\\n",
    "    .drop(l1_dmas_df.action_categ).drop(l1_dmas_df.dt).drop(l1_dmas_df.session_id).drop(l1_dmas_df.item_id).drop(l1_dmas_df.action_sub_categ)\n",
    "    l2_l1_dmas.write.mode(\"overwrite\").partitionBy('dt', 'action_categ', 'action_sub_categ').saveAsTable('fair_dev.yp_test_interactions_l2_l1_dmas')\n",
    "    # session_id, dt, action_categ, us_seller_id, catalog_item_id on l2 table\n",
    "    print('l2_l1_dmas saved for site signals')\n",
    "    \n",
    "    l2_l1_dmas = \"\"\"select * from fair_dev.yp_test_interactions_l2_l1_dmas \n",
    "        WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                        AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                        AND dt NOT IN __bad_days__\"\"\"\n",
    "    l2_l1_dmas = l2_l1_dmas.replace('__today__', today_ds)\n",
    "    l2_l1_dmas = l2_l1_dmas.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    l2_l1_dmas = l2_l1_dmas.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    l2_l1_dmas = l2_l1_dmas.replace('__bad_days__', bad_days)\n",
    "    spark.sql(l2_l1_dmas).createOrReplaceTempView('l2_l1_dmas_df')\n",
    "    \n",
    "    \n",
    "    query = \"\"\"\n",
    "            WITH   \n",
    "            tab_product_view\n",
    "            AS\n",
    "            (SELECT catalog_item_id, us_seller_id AS seller_id,\n",
    "                count(*) AS total_page_views\n",
    "                FROM l2_l1_dmas_df\n",
    "                WHERE action_categ = 'pageView'\n",
    "                GROUP BY catalog_item_id, us_seller_id),\n",
    "            \n",
    "            tab_add_to_cart\n",
    "            AS\n",
    "            (SELECT catalog_item_id, us_seller_id AS seller_id,\n",
    "                count(*) AS total_add_to_carts\n",
    "                FROM l2_l1_dmas_df\n",
    "                WHERE action_sub_categ = 'addToCart'\n",
    "                GROUP BY catalog_item_id, us_seller_id),\n",
    "                \n",
    "            tab_auth1\n",
    "            AS\n",
    "            (SELECT daily_order.catalog_item_id, transaction.slr_org_cd,\n",
    "               daily_order.order_nbr, \n",
    "               SUM(total_auth_amount) AS total_revenue, \n",
    "               SUM(total_auth_cost) AS total_cost \n",
    "               FROM (SELECT * FROM fair.daily_auth_order\n",
    "                  WHERE auth_date >= date_sub('__today__', __daysback_end__)\n",
    "                  AND auth_date <= date_sub('__today__', __daysback_start__)\n",
    "                  AND auth_date NOT IN __bad_days__) daily_order\n",
    "\n",
    "               LEFT JOIN (SELECT order_line_nbr, order_nbr,catlg_item_id,slr_org_cd \n",
    "                           FROM ww_crew_dl_rpt_vm.cnsld_order_item \n",
    "                           WHERE order_plcd_dt>= date_sub('__today__', __daysback_end__)\n",
    "                           AND order_plcd_dt<= date_sub('__today__', __daysback_start__)\n",
    "                           AND order_plcd_dt NOT IN __bad_days__) transaction\n",
    "               ON transaction.order_nbr = daily_order.order_nbr\n",
    "               AND transaction.order_line_nbr = daily_order.line_nbr\n",
    "\n",
    "               GROUP BY daily_order.catalog_item_id, transaction.slr_org_cd, daily_order.order_nbr),\n",
    "\n",
    "            tab_auth2\n",
    "            AS\n",
    "            (SELECT catalog_item_id, slr_org_cd as seller_id,\n",
    "              count(*) AS total_orders,\n",
    "              SUM(total_revenue) AS total_revenue,\n",
    "              SUM(total_cost) AS total_cost,\n",
    "              SUM(total_revenue) - SUM(total_cost) AS total_margin\n",
    "\n",
    "            FROM\n",
    "\n",
    "               tab_auth1\n",
    "\n",
    "            GROUP BY catalog_item_id, slr_org_cd)\n",
    "            \n",
    "                -- final query\n",
    "    \n",
    "            SELECT dctr_pla_distinct.catalog_item_id AS catalog_item_id,\n",
    "                   dctr_pla_distinct.seller_id,\n",
    "                   tab_product_view.total_page_views AS total_page_views,\n",
    "                   tab_add_to_cart.total_add_to_carts AS total_add_to_carts,\n",
    "                   tab_auth2.total_revenue AS total_revenue,\n",
    "                   tab_auth2.total_margin AS margin,\n",
    "                   tab_add_to_cart.total_add_to_carts/tab_product_view.total_page_views AS atcpv,\n",
    "                   tab_auth2.total_orders/tab_product_view.total_page_views AS convrt,\n",
    "                   tab_auth2.total_revenue/tab_product_view.total_page_views AS rpv,\n",
    "                   tab_auth2.total_margin/tab_product_view.total_page_views AS mpv,\n",
    "                   tab_auth2.total_revenue/tab_auth2.total_orders AS ordersize,\n",
    "                   tab_auth2.total_orders AS total_orders\n",
    "\n",
    "            FROM (SELECT DISTINCT catalog_item_id, seller_id from dctr_pla) dctr_pla_distinct \n",
    "                 LEFT JOIN tab_product_view\n",
    "                 ON dctr_pla_distinct.catalog_item_id = tab_product_view.catalog_item_id\n",
    "                 AND dctr_pla_distinct.seller_id = tab_product_view.seller_id\n",
    "                 LEFT JOIN tab_add_to_cart\n",
    "                 ON dctr_pla_distinct.catalog_item_id = tab_add_to_cart.catalog_item_id\n",
    "                 AND dctr_pla_distinct.seller_id = tab_add_to_cart.seller_id\n",
    "                 LEFT JOIN tab_auth2\n",
    "                 ON dctr_pla_distinct.catalog_item_id = tab_auth2.catalog_item_id\n",
    "                 AND dctr_pla_distinct.seller_id = tab_auth2.seller_id\n",
    "            \"\"\"\n",
    "    \n",
    "#     query = \"\"\"\n",
    "#             WITH   \n",
    "#             tab_product_view\n",
    "#             AS\n",
    "#             (SELECT catlg_itm_id_set[0] AS catalog_item_id, seller_id, \n",
    "#                 count(*) AS total_page_views\n",
    "\n",
    "#                 FROM (SELECT * FROM us_dl_interactions_restrict.interactions_l2_session_events\n",
    "#                     WHERE vtc IS NOT NULL\n",
    "#                           AND vtc != ''\n",
    "#                           AND vtc != '\\\\N'\n",
    "#                           AND catlg_itm_id_set[0] IS NOT NULL\n",
    "#                           AND catlg_itm_id_set[0] != ''\n",
    "#                           AND action_categ = 'pageView'\n",
    "#                           AND dt >= date_sub('__today__', __daysback_end__)\n",
    "#                           AND dt <= date_sub('__today__', __daysback_start__)\n",
    "#                           AND dt NOT IN __bad_days__\n",
    "#                           AND pipeline in ('PULSE_USOA_RWEB', 'ANIVIA_USOA_APP') \n",
    "#                           AND context = 'productPage') interactions_l2\n",
    "#                 LEFT JOIN (SELECT distinct session_id, dt, dmas.us_seller_id as seller_id \n",
    "#                             FROM us_dl_interactions_restrict.interactions_l1_cbb_catalog l1\n",
    "#                             INNER JOIN (SELECT distinct seller_id, us_seller_id from dmas.dfm\n",
    "#                                         WHERE ts = '__partitionDate__') dmas\n",
    "#                             ON dt >= date_sub('__today__', __daysback_end__)\n",
    "#                             AND dt <= date_sub('__today__', __daysback_start__)\n",
    "#                             AND dt NOT IN __bad_days__\n",
    "#                             AND itm_list is not null\n",
    "#                             AND itm_list[0]['catlgItm']['sellerId'] IS NOT NULL\n",
    "#                             AND action_categ = 'pageView'\n",
    "#                             AND pipeline in ('PULSE_USOA_RWEB', 'ANIVIA_USOA_APP') \n",
    "#                             AND context = 'productPage'\n",
    "#                             AND l1.itm_list[0]['catlgItm']['sellerId'] = dmas.seller_id) interactions_l1\n",
    "#                 ON interactions_l2.session_id = interactions_l1.session_id\n",
    "#                 AND interactions_l2.dt = interactions_l1.dt\n",
    "                \n",
    "#                 GROUP BY catlg_itm_id_set[0], seller_id),\n",
    "                \n",
    "#             tab_add_to_cart\n",
    "#             AS\n",
    "#             (SELECT catlg_itm_id_set[0] AS catalog_item_id, seller_id, \n",
    "#             count(*) AS total_add_to_carts\n",
    "\n",
    "#         FROM (SELECT * FROM us_dl_interactions_restrict.interactions_l2_session_events\n",
    "#                     WHERE vtc IS NOT NULL\n",
    "#                           AND vtc != ''\n",
    "#                           AND vtc != '\\\\N'\n",
    "#                           AND catlg_itm_id_set[0] IS NOT NULL\n",
    "#                           AND catlg_itm_id_set[0] != ''\n",
    "#                           AND action_categ = 'addToCart'\n",
    "#                           AND dt >= date_sub('__today__', __daysback_end__)\n",
    "#                           AND dt <= date_sub('__today__', __daysback_start__)\n",
    "#                           AND dt NOT IN __bad_days__\n",
    "#                           AND pipeline in ('PULSE_USOA_RWEB', 'ANIVIA_USOA_APP') \n",
    "#                           AND context = 'productPage') interactions_l2\n",
    "#             LEFT JOIN (SELECT distinct session_id, dt, dmas.us_seller_id as seller_id \n",
    "#                             FROM us_dl_interactions_restrict.interactions_l1_cbb_catalog l1\n",
    "#                             INNER JOIN (SELECT distinct seller_id, us_seller_id from dmas.dfm\n",
    "#                                         WHERE ts = '__partitionDate__') dmas\n",
    "#                             ON dt >= date_sub('__today__', __daysback_end__)\n",
    "#                             AND dt <= date_sub('__today__', __daysback_start__)\n",
    "#                             AND dt NOT IN __bad_days__\n",
    "#                             AND itm_list is not null\n",
    "#                             AND itm_list[0]['catlgItm']['sellerId'] IS NOT NULL\n",
    "#                             AND action_categ = 'addToCart'\n",
    "#                             AND pipeline in ('PULSE_USOA_RWEB', 'ANIVIA_USOA_APP') \n",
    "#                             AND context = 'productPage'\n",
    "#                             AND l1.itm_list[0]['catlgItm']['sellerId'] = dmas.seller_id) interactions_l1\n",
    "#             ON interactions_l2.session_id = interactions_l1.session_id\n",
    "#             AND interactions_l2.dt = interactions_l1.dt\n",
    "\n",
    "#         GROUP BY catlg_itm_id_set[0], seller_id),\n",
    "        \n",
    "#         tab_auth1\n",
    "#         AS\n",
    "#         (SELECT daily_order.catalog_item_id, transaction.slr_org_cd,\n",
    "#            daily_order.order_nbr, \n",
    "#            SUM(total_auth_amount) AS total_revenue, \n",
    "#            SUM(total_auth_cost) AS total_cost \n",
    "#            FROM (SELECT * FROM fair.daily_auth_order\n",
    "#               WHERE auth_date >= date_sub('__today__', __daysback_end__)\n",
    "#               AND auth_date <= date_sub('__today__', __daysback_start__)\n",
    "#               AND auth_date NOT IN __bad_days__) daily_order\n",
    "           \n",
    "#            LEFT JOIN (SELECT order_line_nbr, order_nbr,catlg_item_id,slr_org_cd \n",
    "#                        FROM ww_crew_dl_rpt_vm.cnsld_order_item \n",
    "#                        WHERE order_plcd_dt>= date_sub('__today__', __daysback_end__)\n",
    "#                        AND order_plcd_dt<= date_sub('__today__', __daysback_start__)\n",
    "#                        AND order_plcd_dt NOT IN __bad_days__) transaction\n",
    "#            ON transaction.order_nbr = daily_order.order_nbr\n",
    "#            AND transaction.order_line_nbr = daily_order.line_nbr\n",
    "\n",
    "#            GROUP BY daily_order.catalog_item_id, transaction.slr_org_cd, daily_order.order_nbr),\n",
    "           \n",
    "#         tab_auth2\n",
    "#         AS\n",
    "#         (SELECT catalog_item_id, slr_org_cd as seller_id,\n",
    "#           count(*) AS total_orders,\n",
    "#           SUM(total_revenue) AS total_revenue,\n",
    "#           SUM(total_cost) AS total_cost,\n",
    "#           SUM(total_revenue) - SUM(total_cost) AS total_margin\n",
    "          \n",
    "#         FROM\n",
    "  \n",
    "#            tab_auth1\n",
    "           \n",
    "#         GROUP BY catalog_item_id, slr_org_cd)\n",
    "    \n",
    "#     -- final query\n",
    "    \n",
    "#     SELECT dctr_pla_distinct.catalog_item_id AS catalog_item_id,\n",
    "#            dctr_pla_distinct.seller_id,\n",
    "#            tab_product_view.total_page_views AS total_page_views,\n",
    "#            tab_add_to_cart.total_add_to_carts AS total_add_to_carts,\n",
    "#            tab_auth2.total_revenue AS total_revenue,\n",
    "#            tab_auth2.total_margin AS margin,\n",
    "#            tab_add_to_cart.total_add_to_carts/tab_product_view.total_page_views AS atcpv,\n",
    "#            tab_auth2.total_orders/tab_product_view.total_page_views AS convrt,\n",
    "#            tab_auth2.total_revenue/tab_product_view.total_page_views AS rpv,\n",
    "#            tab_auth2.total_margin/tab_product_view.total_page_views AS mpv,\n",
    "#            tab_auth2.total_revenue/tab_auth2.total_orders AS ordersize,\n",
    "#            tab_auth2.total_orders AS total_orders\n",
    "          \n",
    "#     FROM (SELECT DISTINCT catalog_item_id, seller_id from dctr_pla) dctr_pla_distinct \n",
    "#          LEFT JOIN tab_product_view\n",
    "#          ON dctr_pla_distinct.catalog_item_id = tab_product_view.catalog_item_id\n",
    "#          AND dctr_pla_distinct.seller_id = tab_product_view.seller_id\n",
    "#          LEFT JOIN tab_add_to_cart\n",
    "#          ON dctr_pla_distinct.catalog_item_id = tab_add_to_cart.catalog_item_id\n",
    "#          AND dctr_pla_distinct.seller_id = tab_add_to_cart.seller_id\n",
    "#          LEFT JOIN tab_auth2\n",
    "#          ON dctr_pla_distinct.catalog_item_id = tab_auth2.catalog_item_id\n",
    "#          AND dctr_pla_distinct.seller_id = tab_auth2.seller_id\n",
    "        \n",
    "#         \"\"\"\n",
    "    query = query.replace('__today__', today_ds)\n",
    "    query = query.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    query = query.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    query = query.replace('__bad_days__', bad_days)\n",
    "    query = query.replace('__partitionDate__', partitionDate)\n",
    "    sql = spark.sql(query)\n",
    "    df = sql.toPandas()\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siteSignal_item(today_ds, days_backtrack_end, days_backtrack_start, bad_days):\n",
    "    \n",
    "    # this function is to retrieve the \"site\" signals of each item/ad\n",
    "    # site signals are to measure the performance of an item/ad on whole Walmart site (SEM signals only indicate the performance of \"SEM\" channel)\n",
    "    # signals are extracted from different time intervals as specified in the json file - 'output_sem_performance_prediction_config_train_config_20201227.json'\n",
    "    \n",
    "    query = \"\"\"\n",
    "            WITH   \n",
    "            tab_product_view\n",
    "            AS\n",
    "            (SELECT catlg_itm_id_set[0] AS catalog_item_id,\n",
    "                count(*) AS total_page_views\n",
    "\n",
    "                FROM us_dl_interactions_restrict.interactions_l2_session_events \n",
    "\n",
    "                WHERE vtc IS NOT NULL\n",
    "                      AND vtc != ''\n",
    "                      AND vtc != '\\\\N'\n",
    "                      AND catlg_itm_id_set[0] IS NOT NULL\n",
    "                      AND catlg_itm_id_set[0] != ''\n",
    "                      AND action_categ = 'pageView'\n",
    "                      AND dt >= date_sub('__today__', __daysback_end__)\n",
    "                      AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                      AND dt NOT IN __bad_days__\n",
    "                      AND pipeline in ('PULSE_USOA_RWEB', 'ANIVIA_USOA_APP') \n",
    "                      AND context = 'productPage'\n",
    "                GROUP BY catlg_itm_id_set[0]),\n",
    "                \n",
    "            tab_add_to_cart\n",
    "            AS\n",
    "            (SELECT catlg_itm_id_set[0] AS catalog_item_id,\n",
    "            count(*) AS total_add_to_carts\n",
    "\n",
    "        FROM us_dl_interactions_restrict.interactions_l2_session_events\n",
    "\n",
    "        WHERE vtc IS NOT NULL\n",
    "              AND vtc != ''\n",
    "              AND vtc != '\\\\N'\n",
    "              AND catlg_itm_id_set[0] IS NOT NULL\n",
    "              AND catlg_itm_id_set[0] != ''\n",
    "              AND action_sub_categ = 'addToCart'\n",
    "              AND dt >= date_sub('__today__', __daysback_end__)\n",
    "              AND dt <= date_sub('__today__', __daysback_start__)\n",
    "              AND dt NOT IN __bad_days__\n",
    "              AND pipeline in ('PULSE_USOA_RWEB', 'ANIVIA_USOA_APP') \n",
    "              AND context = 'productPage'\n",
    "        GROUP BY catlg_itm_id_set[0]),\n",
    "        \n",
    "        tab_auth1\n",
    "        AS\n",
    "        (SELECT catalog_item_id, \n",
    "           order_nbr, \n",
    "           SUM(total_auth_amount) AS total_revenue, \n",
    "           SUM(total_auth_cost) AS total_cost \n",
    "           FROM fair.daily_auth_order \n",
    "           WHERE \n",
    "              auth_date >= date_sub('__today__', __daysback_end__)\n",
    "              AND auth_date <= date_sub('__today__', __daysback_start__)\n",
    "              AND auth_date NOT IN __bad_days__\n",
    "           GROUP BY catalog_item_id, order_nbr),\n",
    "           \n",
    "        tab_auth2\n",
    "        AS\n",
    "        (SELECT catalog_item_id,\n",
    "          count(*) AS total_orders,\n",
    "          SUM(total_revenue) AS total_revenue,\n",
    "          SUM(total_cost) AS total_cost,\n",
    "          SUM(total_revenue) - SUM(total_cost) AS total_margin\n",
    "          \n",
    "        FROM\n",
    "  \n",
    "           tab_auth1\n",
    "           \n",
    "        GROUP BY catalog_item_id)\n",
    "    \n",
    "    -- final query\n",
    "    \n",
    "    SELECT dctr_pla.catalog_item_id AS catalog_item_id,\n",
    "           dctr_pla.seller_id AS seller_id,\n",
    "           tab_product_view.total_page_views AS total_page_views,\n",
    "           tab_add_to_cart.total_add_to_carts AS total_add_to_carts,\n",
    "           tab_auth2.total_revenue AS total_revenue,\n",
    "           tab_auth2.total_margin AS margin,\n",
    "           tab_add_to_cart.total_add_to_carts/tab_product_view.total_page_views AS atcpv,\n",
    "           tab_auth2.total_orders/tab_product_view.total_page_views AS convrt,\n",
    "           tab_auth2.total_revenue/tab_product_view.total_page_views AS rpv,\n",
    "           tab_auth2.total_margin/tab_product_view.total_page_views AS mpv,\n",
    "           tab_auth2.total_revenue/tab_auth2.total_orders AS ordersize,\n",
    "           tab_auth2.total_orders AS total_orders\n",
    "          \n",
    "    FROM dctr_pla LEFT JOIN tab_product_view\n",
    "         ON dctr_pla.catalog_item_id = tab_product_view.catalog_item_id\n",
    "         LEFT JOIN tab_add_to_cart\n",
    "         ON tab_product_view.catalog_item_id = tab_add_to_cart.catalog_item_id\n",
    "         LEFT JOIN tab_auth2\n",
    "         ON tab_add_to_cart.catalog_item_id = tab_auth2.catalog_item_id\n",
    "        \n",
    "        \"\"\"\n",
    "    query = query.replace('__today__', today_ds)\n",
    "    query = query.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    query = query.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    query = query.replace('__bad_days__', bad_days)\n",
    "    \n",
    "    sql = spark.sql(query)\n",
    "    df = sql.toPandas()\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# func for bounce signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounceSignal(today_ds, days_backtrack_end, days_backtrack_start, bad_days):\n",
    "    \n",
    "    # bounce signals are to measure non-effective visit in SEM channel as well as whole walmart site\n",
    "    # a bounce is a non-effective visit - for example viewed but not added to cart\n",
    "    \n",
    "    partitionDate = spark.sql(\"\"\"show partitions dmas.dfm\"\"\").rdd.flatMap(lambda x: x).map(lambda x: x.replace(\"ts=\", \"\")).max()\n",
    "    \n",
    "    l1 = \"\"\"SELECT distinct session_id, dt, itm_list[0]['catlgItm']['sellerId'] as sellerid, action_categ,\n",
    "                            itm_list[0]['catlgItm']['productId'] as productid\n",
    "                            FROM us_dl_interactions_restrict.interactions_l1_cbb_catalog\n",
    "                            WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                            AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                            AND dt NOT IN __bad_days__\n",
    "                            AND pipeline = 'PULSE_USOA_RWEB'\n",
    "                            AND itm_list is not null\n",
    "                            AND itm_list[0]['catlgItm']['sellerId'] IS NOT NULL\n",
    "                            AND itm_list[0]['catlgItm']['productId'] IS NOT NULL\"\"\"\n",
    "    l1 = l1.replace('__today__', today_ds)\n",
    "    l1 = l1.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    l1 = l1.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    l1 = l1.replace('__bad_days__', bad_days)\n",
    "    l1 = l1.replace('__partitionDate__', partitionDate)\n",
    "    # session_id, dt, action_categ, sellerid, productid\n",
    "    spark.sql(l1).write.mode(\"overwrite\").partitionBy('dt', 'action_categ').saveAsTable('fair_dev.yp_test_bounce_interactions_l1')\n",
    "    \n",
    "    print('l1 saved for bounce signals')\n",
    "    \n",
    "    dmas_seller = spark.sql(\"select * from fair_dev.yp_test_dmas_seller\")\n",
    "    dmas_item = spark.sql(\"select * from fair_dev.yp_test_dmas_item\")\n",
    "    l1 = \"\"\"select * from fair_dev.yp_test_bounce_interactions_l1 \n",
    "            WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                            AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                            AND dt NOT IN __bad_days__\"\"\"\n",
    "    l1 = l1.replace('__today__', today_ds)\n",
    "    l1 = l1.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    l1 = l1.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    l1 = l1.replace('__bad_days__', bad_days)\n",
    "    l1_df = spark.sql(l1)\n",
    "    \n",
    "    l1_dmas = l1_df.join(broadcast(dmas_seller),dmas_seller.seller_id ==  l1_df.sellerid, how='inner').drop(l1_df.sellerid).drop(dmas_seller.seller_id)\n",
    "    l1_dmas = l1_dmas.join(dmas_item,dmas_item.product_id ==  l1_dmas.productid, how='left').drop(dmas_item.product_id).drop(l1_dmas.productid)\n",
    "    # session_id, dt, action_categ, us_seller_id, item_id on table l1\n",
    "    l1_dmas.write.mode(\"overwrite\").partitionBy('dt', 'action_categ').saveAsTable('fair_dev.yp_test_bounce_interactions_l1_dmas')\n",
    "    \n",
    "    print('pageview l1_dmas saved for bounce signals after getting seller_id and catalog_item_id from dmas table')\n",
    "    \n",
    "    l1_dmas = \"\"\"select * from fair_dev.yp_test_bounce_interactions_l1_dmas \n",
    "        WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                        AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                        AND dt NOT IN __bad_days__\"\"\"\n",
    "    l1_dmas = l1_dmas.replace('__today__', today_ds)\n",
    "    l1_dmas = l1_dmas.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    l1_dmas = l1_dmas.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    l1_dmas = l1_dmas.replace('__bad_days__', bad_days)\n",
    "    l1_dmas_df = spark.sql(l1_dmas)\n",
    "    \n",
    "    spark.sql(l1_dmas).createOrReplaceTempView('l1_dmas')\n",
    "    \n",
    "    query = \"\"\"\n",
    "            SELECT dctr_pla_distinct.catalog_item_id,\n",
    "                      dctr_pla_distinct.seller_id, \n",
    "                      SUM(num_bounces) AS bounces,\n",
    "                      SUM(num_visits) AS visits,\n",
    "                      SUM(num_sem_visits) AS sem_visits,\n",
    "                      SUM(num_sem_bounces) AS sem_bounces,\n",
    "                      SUM(num_bounces)/SUM(num_visits) AS bounce_rate,\n",
    "                      SUM(num_sem_bounces)/SUM(num_sem_visits) AS sem_bounce_rate\n",
    "       \n",
    "               FROM\n",
    "               (SELECT DISTINCT session_id, \n",
    "                        mkt_veh, \n",
    "                        total_non_performance_metrics_view,\n",
    "                        1 AS num_visits,\n",
    "                        IF (total_non_performance_metrics_view = 1, 1,  0) AS num_bounces,\n",
    "                        IF (mkt_veh = 'Paid:SEM', 1, 0) AS num_sem_visits,\n",
    "                        IF (total_non_performance_metrics_view = 1 AND mkt_veh = 'Paid:SEM', 1, 0) AS num_sem_bounces\n",
    "        \n",
    "                 FROM us_dl_interactions_restrict.interactions_l3_session_stats_daily l3\n",
    "        \n",
    "                 WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                       AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                       AND dt NOT IN __bad_days__\n",
    "                       AND pipeline = 'PULSE_USOA_RWEB')  cbb_l3\n",
    "            \n",
    "               LEFT JOIN l1_dmas cbb_l1\n",
    "           \n",
    "                ON cbb_l3.session_id = cbb_l1.session_id\n",
    "    \n",
    "                RIGHT JOIN (SELECT DISTINCT catalog_item_id, seller_id from dctr_pla) dctr_pla_distinct \n",
    "    \n",
    "                ON dctr_pla_distinct.catalog_item_id = cbb_l1.item_id\n",
    "                AND dctr_pla_distinct.seller_id = cbb_l1.us_seller_id\n",
    "                \n",
    "                GROUP BY dctr_pla_distinct.catalog_item_id, dctr_pla_distinct.seller_id\n",
    "    \n",
    "        \"\"\" \n",
    "    \n",
    "#     query = \"\"\"WITH \n",
    "#                l2_withsellerid \n",
    "#                AS\n",
    "#                (SELECT DISTINCT l2.session_id, l2.catalog_item_id, l1.seller_id\n",
    "#                 FROM \n",
    "#                 (SELECT DISTINCT catlg_itm_id_set[0] AS catalog_item_id, session_id\n",
    "#                     FROM us_dl_interactions_restrict.interactions_l2_session_events\n",
    "#                     WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "#                     AND dt <= date_sub('__today__', __daysback_start__)\n",
    "#                     AND dt NOT IN __bad_days__\n",
    "#                     AND pipeline = 'PULSE_USOA_RWEB') l2\n",
    "#                 INNER JOIN (\n",
    "#                         SELECT distinct session_id, dt, dmas.us_seller_id as seller_id \n",
    "#                         FROM us_dl_interactions_restrict.interactions_l1_cbb_catalog l1\n",
    "#                         INNER JOIN (\n",
    "#                                     SELECT distinct seller_id, us_seller_id from dmas.dfm\n",
    "#                                     WHERE ts = '__partitionDate__'\n",
    "#                                     ) dmas\n",
    "#                         ON dt >= date_sub('__today__', __daysback_end__)\n",
    "#                         AND dt <= date_sub('__today__', __daysback_start__)\n",
    "#                         AND dt NOT IN __bad_days__\n",
    "#                         AND itm_list is not null\n",
    "#                         AND itm_list[0]['catlgItm']['sellerId'] IS NOT NULL\n",
    "#                         AND pipeline = 'PULSE_USOA_RWEB'\n",
    "#                         AND l1.itm_list[0]['catlgItm']['sellerId'] = dmas.seller_id\n",
    "#                     ) l1_withseller\n",
    "#                 ON l2.session_id = l1_withseller.session_id\n",
    "#                 AND l2.dt = l1_withseller.dt\n",
    "                \n",
    "                \n",
    "#                 -- final query\n",
    "    \n",
    "#             SELECT dctr_pla_distinct.catalog_item_id,\n",
    "#                       dctr_pla_distinct.seller_id, \n",
    "#                       SUM(num_bounces) AS bounces,\n",
    "#                       SUM(num_visits) AS visits,\n",
    "#                       SUM(num_sem_visits) AS sem_visits,\n",
    "#                       SUM(num_sem_bounces) AS sem_bounces,\n",
    "#                       SUM(num_bounces)/SUM(num_visits) AS bounce_rate,\n",
    "#                       SUM(num_sem_bounces)/SUM(num_sem_visits) AS sem_bounce_rate\n",
    "       \n",
    "#                FROM\n",
    "#                (SELECT DISTINCT session_id, \n",
    "#                         mkt_veh, \n",
    "#                         total_non_performance_metrics_view,\n",
    "#                         1 AS num_visits,\n",
    "#                         IF (total_non_performance_metrics_view = 1, 1,  0) AS num_bounces,\n",
    "#                         IF (mkt_veh = 'Paid:SEM', 1, 0) AS num_sem_visits,\n",
    "#                         IF (total_non_performance_metrics_view = 1 AND mkt_veh = 'Paid:SEM', 1, 0) AS num_sem_bounces\n",
    "        \n",
    "#                  FROM us_dl_interactions_restrict.interactions_l3_session_stats_daily l3\n",
    "        \n",
    "#                  WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "#                        AND dt <= date_sub('__today__', __daysback_start__)\n",
    "#                        AND dt NOT IN __bad_days__\n",
    "#                        AND pipeline = 'PULSE_USOA_RWEB')  cbb_l3\n",
    "            \n",
    "#                LEFT JOIN l2_withsellerid cbb_l2\n",
    "           \n",
    "#                 ON cbb_l3.session_id = cbb_l2.session_id\n",
    "    \n",
    "#                 RIGHT JOIN (SELECT DISTINCT catalog_item_id, seller_id from dctr_pla) dctr_pla_distinct \n",
    "    \n",
    "#                 ON dctr_pla_distinct.catalog_item_id = cbb_l2.catalog_item_id\n",
    "#                 AND dctr_pla_distinct.seller_id = cbb_l2.seller_id\n",
    "                \n",
    "#                 GROUP BY dctr_pla_distinct.catalog_item_id, dctr_pla_distinct.seller_id\n",
    "    \n",
    "#         \"\"\" \n",
    "    \n",
    "    query = query.replace('__today__', today_ds)\n",
    "    query = query.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    query = query.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    query = query.replace('__bad_days__', bad_days)\n",
    "    query = query.replace('__partitionDate__', partitionDate)\n",
    "    \n",
    "    sql = spark.sql(query)\n",
    "    df = sql.toPandas()\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounceSignal_item(today_ds, days_backtrack_end, days_backtrack_start, bad_days):\n",
    "    \n",
    "    # bounce signals are to measure non-effective visit in SEM channel as well as whole walmart site\n",
    "    # a bounce is a non-effective visit - for example viewed but not added to cart\n",
    "    query = \"\"\"\n",
    "               WITH \n",
    "               l3\n",
    "               AS\n",
    "               (SELECT DISTINCT session_id, \n",
    "                        mkt_veh, \n",
    "                        total_non_performance_metrics_view,\n",
    "                        1 AS num_visits,\n",
    "                        IF (total_non_performance_metrics_view = 1, 1,  0) AS num_bounces,\n",
    "                        IF (mkt_veh = 'Paid:SEM', 1, 0) AS num_sem_visits,\n",
    "                        IF (total_non_performance_metrics_view = 1 AND mkt_veh = 'Paid:SEM', 1, 0) AS num_sem_bounces\n",
    "        \n",
    "                 FROM us_dl_interactions_restrict.interactions_l3_session_stats_daily\n",
    "        \n",
    "                 WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                       AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                       AND dt NOT IN __bad_days__\n",
    "                       AND pipeline = 'PULSE_USOA_RWEB'),\n",
    "                       \n",
    "                l3_item\n",
    "                AS\n",
    "                (SELECT catlg_itm_id_set[0] AS catalog_item_id,\n",
    "                      SUM(num_bounces) AS bounces,\n",
    "                      SUM(num_visits) AS visits,\n",
    "                      SUM(num_sem_visits) AS sem_visits,\n",
    "                      SUM(num_sem_bounces) AS sem_bounces,\n",
    "                      SUM(num_bounces)/SUM(num_visits) AS bounce_rate,\n",
    "                      SUM(num_sem_bounces)/SUM(num_sem_visits) AS sem_bounce_rate\n",
    "                     \n",
    "                FROM (SELECT DISTINCT catlg_itm_id_set, session_id from \n",
    "                      us_dl_interactions_restrict.interactions_l2_session_events\n",
    "                      WHERE dt >= date_sub('__today__', __daysback_end__)\n",
    "                      AND dt <= date_sub('__today__', __daysback_start__)\n",
    "                      AND dt NOT IN __bad_days__\n",
    "                      AND pipeline in ('PULSE_USOA_RWEB')\n",
    "                      AND context = 'productPage') l2\n",
    "                RIGHT JOIN l3\n",
    "                ON l2.session_id = l3.session_id\n",
    "                GROUP BY catlg_itm_id_set[0])\n",
    "                \n",
    "                -- final query\n",
    "                \n",
    "                SELECT dctr_pla.catalog_item_id, \n",
    "                       dctr_pla.seller_id,\n",
    "                       l3_item.bounces,\n",
    "                       l3_item.visits,\n",
    "                       l3_item.sem_visits,\n",
    "                       l3_item.sem_bounces,\n",
    "                       l3_item.bounce_rate,\n",
    "                       l3_item.sem_bounce_rate\n",
    "                FROM dctr_pla\n",
    "                LEFT JOIN l3_item\n",
    "                ON dctr_pla.catalog_item_id = l3_item.catalog_item_id\n",
    "    \n",
    "        \"\"\" \n",
    "    \n",
    "    query = query.replace('__today__', today_ds)\n",
    "    query = query.replace('__daysback_end__', str(days_backtrack_end))\n",
    "    query = query.replace('__daysback_start__', str(days_backtrack_start))\n",
    "    query = query.replace('__bad_days__', bad_days)\n",
    "    \n",
    "    sql = spark.sql(query)\n",
    "    df = sql.toPandas()\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023-02-15'\n",
    "gcp_directory = 'gs://msc_fair_airflow/rpc_model/'\n",
    "ref_dates = []\n",
    "for x in range(0, 5):\n",
    "      ref_dates.append(str(datetime.strptime(start_date, '%Y-%m-%d').date() - timedelta(days=14)*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-02-15', '2023-02-01', '2023-01-18', '2023-01-04', '2022-12-21']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_json(gcp_directory+ 'rpc_time_window_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'com.walmartlabs.crm.unifiers.sem.SemSignalsUnifier': [{'name': 'l',\n",
       "    'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 0 + 6) AND date_sub('2020-12-25', 0) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'w1',\n",
       "    'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 7 + 6) AND date_sub('2020-12-25', 7) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'w2',\n",
       "    'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 7 + 13) AND date_sub('2020-12-25', 7) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'v2',\n",
       "    'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 14 + 13) AND date_sub('2020-12-25', 14) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'v3',\n",
       "    'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 21 + 13) AND date_sub('2020-12-25', 21) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'v4',\n",
       "    'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 28 + 13) AND date_sub('2020-12-25', 28) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'v5',\n",
       "    'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 35 + 13) AND date_sub('2020-12-25', 35) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'v9',\n",
       "    'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 63 + 13) AND date_sub('2020-12-25', 63) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'v12',\n",
       "    'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 84 + 13) AND date_sub('2020-12-25', 84) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'v18',\n",
       "    'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 126 + 13) AND date_sub('2020-12-25', 126) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"}]},\n",
       " {'com.walmartlabs.crm.unifiers.sem.CatalogSignalsUnifier': [{'name': 'w1',\n",
       "    'condition': \"ds BETWEEN date_sub('2020-12-25', 7 + 6) AND date_sub('2020-12-25', 7) AND ds NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"}]},\n",
       " {'com.walmartlabs.crm.unifiers.sem.SiteSignalsUnifier': [{'name': 'w2',\n",
       "    'condition': \"ds BETWEEN date_sub('2020-12-25', 7 + 13) AND date_sub('2020-12-25', 7) AND ds NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'w1',\n",
       "    'condition': \"ds BETWEEN date_sub('2020-12-25', 7 + 6) AND date_sub('2020-12-25', 7) AND ds NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"}]},\n",
       " {'com.walmartlabs.crm.unifiers.sem.BounceSignalsUnifier': [{'name': 'w2',\n",
       "    'condition': \"ds BETWEEN date_sub('2020-12-25', 7 + 13) AND date_sub('2020-12-25', 7) AND ds NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"},\n",
       "   {'name': 'w1',\n",
       "    'condition': \"ds BETWEEN date_sub('2020-12-25', 7 + 6) AND date_sub('2020-12-25', 7) AND ds NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"}]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad days are typically holidays when RPC/target value pattern are very different from normal days to be excluded from training data\n",
    "bad_days = \"\"\"('2022-11-07', '2022-11-14', '2022-11-21', '2022-11-23', '2022-11-24', '2022-11-25')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop through configuration files to get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_sem = list(config[0].values())[0]\n",
    "config_catalog = list(config[1].values())[0]\n",
    "config_site = list(config[2].values())[0]\n",
    "config_bounce = list(config[3].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmas table renew done\n",
      "Querying data for 2022-10-16\n",
      "querying sem signals...\n",
      "querying site signals...\n",
      "processing site features for date 2022-10-16 and name is w2\n",
      "l2 saved for site signals\n"
     ]
    }
   ],
   "source": [
    "# starttime = time.time()\n",
    "# partitionDate = spark.sql(\"\"\"show partitions dmas.dfm\"\"\").rdd.flatMap(lambda x: x).map(lambda x: x.replace(\"ts=\", \"\")).max()\n",
    "\n",
    "# bb = \"\"\"SELECT distinct seller_id, us_seller_id from dmas.dfm WHERE ts = '__partitionDate__'\"\"\"\n",
    "# bb = bb.replace('__partitionDate__', partitionDate)\n",
    "# spark.sql(bb).write.mode(\"overwrite\").saveAsTable('fair_dev.yp_test_dmas_seller')\n",
    "\n",
    "# bb = \"\"\"SELECT distinct item_id, product_id from dmas.dfm WHERE ts = '__partitionDate__'\"\"\"\n",
    "# bb = bb.replace('__partitionDate__', partitionDate)\n",
    "# spark.sql(bb).write.mode(\"overwrite\").saveAsTable('fair_dev.yp_test_dmas_item')\n",
    "\n",
    "# endtime = time.time()\n",
    "# print('time used for creating dmas copy table'+': ' + str(endtime - starttime))\n",
    "\n",
    "for today_ds in ref_dates[:1]:\n",
    "    starttime = time.time()\n",
    "    print('Querying data for %s' % today_ds)\n",
    "    df_feat_all = pd.DataFrame()\n",
    "    \n",
    "    # create table view of all catalog_item_ids of interest (today_ds -139 :  today_ds-7)\n",
    "    join_dctr_item_id(today_ds)\n",
    "    \n",
    "    ###############################################################\n",
    "    #*********************** SEM signals***************************\n",
    "    \n",
    "    # get target values\n",
    "    ##{'name': 'l',\n",
    "    #'condition': \"date_string_p BETWEEN date_sub('2020-12-25', 0 + 6) AND date_sub('2020-12-25', 0) AND date_string_p NOT IN ('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"}\n",
    "\n",
    "    config_tmp = config_sem[0]     \n",
    "    name = config_tmp['name']\n",
    "    condition = config_tmp['condition']\n",
    "    \n",
    "    #    [\"('2020-12-25', 0 + 6)\",\n",
    "    # \"('2020-12-25', 0)\",\n",
    "    # \"('2019-11-30', '2019-11-28', '2019-11-29', '2019-11-27', '2020-02-25', '2020-02-26', '2020-02-27', '2019-12-03', '2019-12-02', '2019-12-01')\"]\n",
    "    backdays = re.findall(r'\\(.*?\\)', condition) #\n",
    "    \n",
    "    days_backtrack_end = backdays[0][1:-1].split(',')[1].split('+')\n",
    "    days_backtrack_end = int(days_backtrack_end[0]) + int(days_backtrack_end[1])\n",
    "    \n",
    "    days_backtrack_start = int(backdays[1][1:-1].split(',')[1])\n",
    "    \n",
    "    # ordersize               double,\n",
    "    # convrt                  double,\n",
    "    # catalog_item_id         long,\n",
    "    # adid                    long,\n",
    "    df_tmp = get_semSignalCoop(today_ds, days_backtrack_end, days_backtrack_start, bad_days)\n",
    "    df_tmp = df_tmp[['catalog_item_id', 'id_unique_variant', 'seller_id', 'brand_nm', 'adid', 'source_id', 'is_mobile','division_id',\\\n",
    "                    'super_department_id','department_id','category_id','sub_category_id']].copy()\n",
    "\n",
    "    upload_csv_to_cloud_storage(df_tmp, gcp_directory+'sample_data_individual_feat/df_header_'+today_ds+'.csv')\n",
    "    del df_tmp\n",
    "    endtime = time.time()\n",
    "    print('time used for creating header table for date '+today_ds+': ' + str(endtime - starttime))\n",
    "    \n",
    "    \n",
    "    for idx in tqdm(range(len(config_sem))):\n",
    "        print('processing sem and sem_coop features for date '+today_ds+' and name is '+config_sem[idx]['name'])\n",
    "        config_tmp = config_sem[idx] \n",
    "        name = config_tmp['name']\n",
    "        condition = config_tmp['condition']\n",
    "    \n",
    "        backdays = re.findall(r'\\(.*?\\)', condition) #\n",
    "    \n",
    "        days_backtrack_end = backdays[0][1:-1].split(',')[1].split('+')\n",
    "        days_backtrack_end = int(days_backtrack_end[0]) + int(days_backtrack_end[1])\n",
    "    \n",
    "        days_backtrack_start = int(backdays[1][1:-1].split(',')[1])\n",
    "    \n",
    "        # sem_is_mobile_w2        int,\n",
    "        # sem_impressions_w2      double,\n",
    "        # sem_clicks_w2           double,\n",
    "        # sem_adspend_w2          double,\n",
    "        # sem_qty_w2              double,\n",
    "        # sem_revenue_w2          double,\n",
    "        # sem_orders_w2           double,\n",
    "        # sem_margin_w2           double,\n",
    "        # sem_ctr_w2              double,\n",
    "        # sem_cpc_w2              double,\n",
    "        # sem_rpc_w2              double,\n",
    "        # sem_roas_w2             double,\n",
    "        # sem_convrt_w2           double,\n",
    "        # sem_ordersize_w2        double,\n",
    "        # sem_mpc_w2              double,\n",
    "        # sem_mpo_w2              double, \n",
    "#         df_tmp = get_semSignal(today_ds, days_backtrack_end, days_backtrack_start, bad_days)\n",
    "        \n",
    "#         df_tmp = df_tmp[['catalog_item_id', 'seller_id', 'source_id', 'adid', 'is_mobile', 'impressions', 'clicks', 'adspend', 'qty', 'gmv_revenue_adjustment', 'orders', 'margin',\\\n",
    "#                             'ctr', 'cpc', 'rpc', 'roas', 'convrt', 'ordersize', 'mpc', 'mpo']].copy()\n",
    "\n",
    "#         columns = df_tmp.columns\n",
    "#         for col in columns[5:]:\n",
    "#             df_tmp.rename(columns = {col: 'sem_'+col+'_'+name}, inplace=True)\n",
    "        \n",
    "#         upload_csv_to_cloud_storage(df_tmp, gcp_directory+'sample_data_individual_feat/sem_'+name+'_'+today_ds+'.csv')\n",
    "#         del df_tmp\n",
    "#         endtime = time.time()\n",
    "#         print('time used for sem signals for evergreen for date '+today_ds+' and name is '+config_sem[idx]['name']+': ' + str(endtime - starttime))\n",
    "        \n",
    "        # get sem coop features\n",
    "        starttime = time.time()\n",
    "        df_tmp = get_semSignalCoop(today_ds, days_backtrack_end, days_backtrack_start, bad_days)\n",
    "\n",
    "        df_tmp = df_tmp[['catalog_item_id', 'seller_id', 'source_id', 'adid', 'is_mobile', 'impressions', 'clicks', 'adspend', 'qty', 'gmv_revenue_adjustment_brand','cost',\\\n",
    "                         'gmv_revenue_adjustment_seller', 'orders_brand', 'orders_seller', 'margin_brand', 'margin_seller',\\\n",
    "                    'ctr', 'cpc', 'rpc_brand', 'rpc_seller', 'roas_brand', 'roas_seller', 'convrt_brand', 'convrt_seller', 'ordersize_brand', 'ordersize_seller',\\\n",
    "                         'mpc_brand', 'mpc_seller', 'mpo_brand', 'mpo_seller']].copy()\n",
    "\n",
    "        columns = df_tmp.columns\n",
    "        for col in columns[5:]:\n",
    "            df_tmp.rename(columns = {col: 'sem_'+col+'_'+name}, inplace=True)\n",
    "        \n",
    "        upload_csv_to_cloud_storage(df_tmp, gcp_directory+'sample_data_individual_feat/semcoop_'+name+'_'+today_ds+'.csv')\n",
    "        del df_tmp\n",
    "        endtime = time.time()\n",
    "        print('time used for sem signals for coop for date '+today_ds+' and name is '+config_sem[idx]['name']+': ' + str(endtime - starttime))\n",
    "        \n",
    "    ###############################################################\n",
    "    #*********************** catalog signals***********************\n",
    "    \n",
    "    print('querying catalog signals...')\n",
    "    for config_tmp in config_catalog:\n",
    "        print('processing catalog features for date '+today_ds+' and name is '+config_tmp['name'])\n",
    "        starttime = time.time()\n",
    "        name = config_tmp['name']\n",
    "        condition = config_tmp['condition']\n",
    "    \n",
    "        backdays = re.findall(r'\\(.*?\\)', condition) #\n",
    "    \n",
    "        days_backtrack_end = backdays[0][1:-1].split(',')[1].split('+')\n",
    "        days_backtrack_end = int(days_backtrack_end[0]) + int(days_backtrack_end[1])\n",
    "    \n",
    "        days_backtrack_start = int(backdays[1][1:-1].split(',')[1])\n",
    "    \n",
    "        # uber_curr_item_price_w1  double,\n",
    "        # uber_num_appr_reviews_w1 double,\n",
    "        # uber_avg_overall_rating_w1 double,\n",
    "        # uber_price_change_ratio_w1 double,\n",
    "        # uber_price_change_value_w1 double,\n",
    "        # uber_avg_margin_w1         double,\n",
    "        \n",
    "        # \"uber_division_w1\"\n",
    "        # \"uber_super_dept_w1\"\n",
    "        # \"uber_dept_w1\"\n",
    "        # \"uber_cat_w1\"\n",
    "        # \"uber_subcat_w1\"\n",
    "    \n",
    "        df_tmp = get_catalogSignal(today_ds, days_backtrack_end, days_backtrack_start, bad_days)\n",
    "        \n",
    "\n",
    "        df_tmp = df_tmp[['catalog_item_id', 'seller_id', 'division','super_dept','dept','cat','subcat','curr_item_price', 'num_appr_reviews', 'avg_overall_rating', 'price_change_ratio',\\\n",
    "                            'price_change_value', 'avg_margin']].copy()\n",
    "\n",
    "        columns = df_tmp.columns\n",
    "        for col in columns[2:]:\n",
    "            df_tmp.rename(columns={col: 'uber_'+col+'_'+name}, inplace=True)\n",
    "        \n",
    "        upload_csv_to_cloud_storage(df_tmp, gcp_directory+'sample_data_individual_feat/catalog_'+name+'_'+today_ds+'.csv')\n",
    "        del df_tmp\n",
    "        endtime = time.time()\n",
    "        print('time used for catalog signals for date '+today_ds+' and name is '+config_tmp['name']+': ' + str(endtime - starttime))\n",
    "\n",
    "    ###############################################################\n",
    "    #*********************** site signals**************************\n",
    "    print('querying site signals...')\n",
    "    for config_tmp in config_site:\n",
    "        name = config_tmp['name']\n",
    "        condition = config_tmp['condition']\n",
    "    \n",
    "        backdays = re.findall(r'\\(.*?\\)', condition) #\n",
    "    \n",
    "        days_backtrack_end = backdays[0][1:-1].split(',')[1].split('+')\n",
    "        days_backtrack_end = int(days_backtrack_end[0]) + int(days_backtrack_end[1])\n",
    "    \n",
    "        days_backtrack_start = int(backdays[1][1:-1].split(',')[1])\n",
    "    \n",
    "#         # site_page_views_w2         double,\n",
    "#         # site_add_to_carts_w2       double,\n",
    "#         # site_orders_w2             double,\n",
    "#         # site_revenue_w2            double,\n",
    "#         # site_atcpv_w2              double,\n",
    "#         # site_convrt_w2             double,\n",
    "#         # site_rpv_w2                double,\n",
    "#         # site_mpv_w2                double,\n",
    "    \n",
    "#         df_tmp = get_siteSignal(today_ds, days_backtrack_end, days_backtrack_start, bad_days)\n",
    "        \n",
    "#         df_tmp = df_tmp[['catalog_item_id', 'seller_id', 'total_page_views', 'total_add_to_carts', 'total_orders', 'total_revenue',\\\n",
    "#                             'atcpv', 'convrt', 'rpv', 'mpv']].copy()\n",
    "#         columns = df_tmp.columns\n",
    "#         for col in columns[2:]:\n",
    "#             df_tmp.rename(columns={col: 'site_'+col+'_'+name}, inplace=True)\n",
    "        \n",
    "#         df_tmp.to_csv('../rpc_model/sample_data_individual_feat/site_'+name+'_'+today_ds+'.csv')\n",
    "#         del df_tmp\n",
    "#         endtime = time.time()\n",
    "#         print('time used for site offer level signals for date '+today_ds+' and name is '+config_tmp['name']+': ' + str(endtime - starttime))\n",
    "        \n",
    "        print('processing site item level features for date '+today_ds+' and name is '+config_tmp['name'])\n",
    "        starttime = time.time()\n",
    "        df_tmp = get_siteSignal_item(today_ds, days_backtrack_end, days_backtrack_start, bad_days)\n",
    "        df_tmp = df_tmp[['catalog_item_id', 'seller_id', 'margin', 'total_page_views', 'total_add_to_carts', 'total_orders', 'total_revenue',\\\n",
    "                            'atcpv', 'convrt', 'rpv', 'mpv']].copy()\n",
    "        columns = df_tmp.columns\n",
    "        for col in columns[2:]:\n",
    "            df_tmp.rename(columns={col: 'site_'+col+'_'+name}, inplace=True)\n",
    "        \n",
    "        upload_csv_to_cloud_storage(df_tmp, gcp_directory+'sample_data_individual_feat/site_item_'+name+'_'+today_ds+'.csv')\n",
    "        del df_tmp\n",
    "        endtime = time.time()\n",
    "        print('time used for site item level signals for date '+today_ds+' and name is '+config_tmp['name']+': ' + str(endtime - starttime))\n",
    "    ###############################################################\n",
    "    #*********************** bounce signals**************************\n",
    "    print('querying bounce signals...')\n",
    "    for config_tmp in config_bounce:\n",
    "        name = config_tmp['name']\n",
    "        condition = config_tmp['condition']\n",
    "    \n",
    "        backdays = re.findall(r'\\(.*?\\)', condition) #\n",
    "    \n",
    "        days_backtrack_end = backdays[0][1:-1].split(',')[1].split('+')\n",
    "        days_backtrack_end = int(days_backtrack_end[0]) + int(days_backtrack_end[1])\n",
    "    \n",
    "        days_backtrack_start = int(backdays[1][1:-1].split(',')[1])\n",
    "    \n",
    "#         # bc_bounce_rate_w1\n",
    "#         # bc_bounces_w1\n",
    "#         # bc_sem_bounce_rate_w1\n",
    "#         # bc_sem_bounces_w1\n",
    "#         # bc_sem_visits_w1\n",
    "#         # bc_visits_w1\n",
    "\n",
    "#         df_tmp = get_bounceSignal(today_ds, days_backtrack_end, days_backtrack_start, bad_days)\n",
    "\n",
    "#         df_tmp = df_tmp[['catalog_item_id', 'seller_id', 'bounce_rate', 'bounces', 'sem_bounce_rate', 'sem_bounces',\\\n",
    "#                             'visits', 'sem_visits']].copy()\n",
    "#         columns = df_tmp.columns\n",
    "#         for col in columns[2:]:\n",
    "#             df_tmp.rename(columns={col: 'bc_'+col+'_'+name}, inplace=True)\n",
    "        \n",
    "#         df_tmp.to_csv('../rpc_model/sample_data_individual_feat/bounce_'+name+'_'+today_ds+'.csv')\n",
    "#         del df_tmp\n",
    "#         endtime = time.time()\n",
    "#         print('time used for bounce offer signals for date '+today_ds+' and name is '+config_tmp['name']+': ' + str(endtime - starttime))\n",
    "        print('processing bounce item features for date '+today_ds+' and name is '+config_tmp['name'])\n",
    "        starttime = time.time()\n",
    "        df_tmp = get_bounceSignal_item(today_ds, days_backtrack_end, days_backtrack_start, bad_days)\n",
    "\n",
    "        df_tmp = df_tmp[['catalog_item_id', 'seller_id', 'bounce_rate', 'bounces', 'sem_bounce_rate', 'sem_bounces',\\\n",
    "                            'visits', 'sem_visits']].copy()\n",
    "        columns = df_tmp.columns\n",
    "        for col in columns[2:]:\n",
    "            df_tmp.rename(columns={col: 'bc_'+col+'_'+name}, inplace=True)\n",
    "        \n",
    "        upload_csv_to_cloud_storage(df_tmp, gcp_directory+'sample_data_individual_feat/bounce_item_'+name+'_'+today_ds+'.csv')\n",
    "        del df_tmp\n",
    "        endtime = time.time()\n",
    "        print('time used for bounce item signals for date '+today_ds+' and name is '+config_tmp['name']+': ' + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_local_to_cloud_storage('/home/y0c07y1/rpc_feature_extraction/query/test.csv', 'gs://msc_fair_airflow/kk/test1.csv')\n",
    "\n",
    "# client = storage.Client()\n",
    "# bucket = client.get_bucket('msc_fair_airflow')\n",
    "# # list all objects in the directory\n",
    "# blobs = bucket.list_blobs(prefix='logs')\n",
    "# for blob in blobs:\n",
    "#     blob.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# today_ds = '2022-03-21'\n",
    "\n",
    "# # insert data into table\n",
    "# pd_df = pd.read_csv('data_lia_37/df_feat_all_'+today_ds+'.csv', index_col=0)\n",
    "# pd_df['adid'] = pd_df['adid'].astype('str')\n",
    "        \n",
    "# sparkDF=spark.createDataFrame(pd_df) \n",
    "# sparkDF.registerTempTable(\"feat\") \n",
    "\n",
    "# query = \"\"\"INSERT INTO TABLE casesci_sem.lia_rpc_feature_test_notxt_cfl\n",
    "#                 PARTITION(date_string = '__today__')\n",
    "#                 SELECT * FROM feat\"\"\"\n",
    "\n",
    "# query = query.replace('__today__', today_ds)\n",
    "# spark.sql(query)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for config_tmp in config_site:\n",
    "            \n",
    "#         name = config_tmp['name']\n",
    "#         condition = config_tmp['condition']\n",
    "    \n",
    "#         backdays = re.findall(r'\\(.*?\\)', condition) #\n",
    "    \n",
    "#         days_backtrack_end = backdays[0][1:-1].split(',')[1].split('+')\n",
    "#         days_backtrack_end = int(days_backtrack_end[0]) + int(days_backtrack_end[1])\n",
    "    \n",
    "#         days_backtrack_start = int(backdays[1][1:-1].split(',')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
