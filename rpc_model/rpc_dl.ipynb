{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql.functions import broadcast\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "os.chdir('/home/y0c07y1/rpc_feature_extraction/rpc_model/')\n",
    "from utils.utils import *\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import functions as func\n",
    "import pyspark.sql as pss\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    MapType,\n",
    "    LongType,\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    IntegerType,\n",
    "    ArrayType\n",
    ")\n",
    "#spark = pss.SparkSession.builder.appName(\"dctr_test3\").config(conf=pyspark.SparkConf().setAll([('spark.yarn.queue', 'root.critical')])).enableHiveSupport().getOrCreate()\n",
    "def gen_spark_session(ds):\n",
    "    ds_nodash = ds.replace(\"-\", \"\")\n",
    "\n",
    "    return pyspark.sql.SparkSession.builder.config(\n",
    "            \"hive.exec.dynamic.partition\", True,\n",
    "        ).config(\n",
    "            \"hive.exec.dynamic.partition.mode\", \"nonstrict\"\n",
    "        ).config('mapreduce.input.fileinputformat.input.dir.recursive',True\n",
    "        ).config('spark.hive.mapred.supports.subdirectories',True\n",
    "        ).config(\"spark.sql.crossJoin.enabled\", \"true\"\n",
    "        ).config(\"spark.sql.broadcastTimeout\", \"36000\"\n",
    "        ).config(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\"\n",
    "        ).config(\"spark.hadoop.orc.overwrite.output.file\",True\n",
    "        ).config(\"spark.yarn.executor.memoryOverhead\", '20g'\n",
    "        ).config(\"spark.executor.memory\",'12g'\n",
    "        ).config(\"spark.driver.memory\",'12g'\n",
    "        ).config(\"spark.dynamicAllocation.enabled\",'true'\n",
    "        ).config(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",'true'\n",
    "        ).config(\"spark.sql.hive.convertMetastoreOrc\", 'false'\n",
    "        ).appName(APP_NAME % ds_nodash).enableHiveSupport().getOrCreate()\n",
    "\n",
    "\n",
    "APP_NAME = 'rpc_feature_extractionjupyter_%s'\n",
    "spark = gen_spark_session('2022-12/7')\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/anaconda/lib/python3.6/site-packages/distributed/config.py:63: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config.update(yaml.load(text) or {})\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import HiveContext, SparkSession\n",
    "from pyspark.sql.functions import col, struct, to_json, udf, expr\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import sys, os, json, shutil, glob, warnings, gc, random, collections, csv, re, logging, time, pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import gcsfs\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmae(y, y_pred, weights):\n",
    "    error = np.abs(y - y_pred)*weights\n",
    "    return np.sum(error)/np.sum(weights)\n",
    "def wrmse(y, y_pred, weights):\n",
    "    error = np.power(y - y_pred, 2)*weights\n",
    "    return np.sqrt(np.sum(error)/np.sum(weights))\n",
    "def train(df_trn, df_val, feature_cols, target_col, lgb_params, model_name, weight_col=None):\n",
    "    # build dataset\n",
    "    if weight_col:\n",
    "        dtrain = lgb.Dataset(df_trn[feature_cols],\n",
    "                             df_trn[target_col].values,\n",
    "                             weight=df_trn[weight_col].values,\n",
    "                             #weight=np.log(df_trn[weight_col].values + np.e),\n",
    "                             free_raw_data=False,\n",
    "                             silent=False,\n",
    "                             )\n",
    "        dvalid = lgb.Dataset(df_val[feature_cols],\n",
    "                             df_val[target_col].values,\n",
    "                             weight=df_val[weight_col].values,\n",
    "                             #weight=np.log(df_val[weight_col].values + np.e),\n",
    "                             free_raw_data=False,\n",
    "                             silent=False,\n",
    "                             )\n",
    "    # train model\n",
    "    model = lgb.train(lgb_params,\n",
    "                      dtrain,\n",
    "                      valid_sets=[dtrain, dvalid],\n",
    "                      verbose_eval=100)\n",
    "\n",
    "    local_file = f'dp_output/{model_name}.txt'\n",
    "    model.save_model(local_file, num_iteration=model.best_iteration)\n",
    "\n",
    "\n",
    "def eval(df, feature_cols, target_col, model_name, weight_col=None):\n",
    "    try:\n",
    "        model = lgb.Booster(model_file=(f'dp_output/{model_name}.txt'))\n",
    "    except:\n",
    "        raise RuntimeError(f'{model_name} does not exist...')\n",
    "    y_pred = model.predict(df[feature_cols])\n",
    "    y_pred = np.array([max(0,x) if np.isnan(x)==False else x for x in y_pred])\n",
    "    WRMSE = wrmse(df[target_col].values, y_pred, df[weight_col].values)\n",
    "    WMAE = wmae(df[target_col].values, y_pred, df[weight_col].values)\n",
    "    return y_pred\n",
    "\n",
    "lgb_params = {\n",
    "'n_estimators': 4000,\n",
    "'metric': 'rmse',\n",
    "'learning_rate': 0.02,\n",
    "'objective': 'regression',\n",
    "'boosting_type': 'gbdt',\n",
    "'subsample': 0.9,\n",
    "'bagging_freq': 10,\n",
    "'colsample_bytree': 0.33,\n",
    "'verbose': -1,\n",
    "'num_leaves': 63,\n",
    "'max_depth': 10,\n",
    "'seed': 42,\n",
    "'n_jobs': -1,\n",
    "'early_stopping_rounds': 200,\n",
    "}\n",
    "config = load_json('gs://msc_fair_airflow/rpc_model/config_rpc.json')\n",
    "adid_name = config[\"hash_id\"][\"adid\"]\n",
    "is_mobile_name = config[\"hash_id\"][\"is_mobile\"]\n",
    "source_id_name = config[\"hash_id\"][\"source_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gs://msc_fair_airflow/rpc_model/sample_data_individual_feat/catalog_w1_2023-01-16.csv', usecols = ['uber_division_w1','uber_super_dept_w1',\n",
    "                                                                                                                         'uber_dept_w1','uber_cat_w1',\n",
    "                                                                                                                         'uber_subcat_w1','seller_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('gs://msc_fair_airflow/rpc_model/sample_data_individual_feat/catalog_w1_2023-01-16.csv',nrows = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'catalog_item_id', 'seller_id', 'uber_division_w1',\n",
       "       'uber_super_dept_w1', 'uber_dept_w1', 'uber_cat_w1', 'uber_subcat_w1',\n",
       "       'uber_curr_item_price_w1', 'uber_num_appr_reviews_w1',\n",
       "       'uber_avg_overall_rating_w1', 'uber_price_change_ratio_w1',\n",
       "       'uber_price_change_value_w1', 'uber_avg_margin_w1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uber_division_w1 10\n",
      "uber_super_dept_w1 60\n",
      "uber_dept_w1 529\n",
      "uber_cat_w1 1635\n",
      "uber_subcat_w1 5760\n",
      "seller_id 39813\n"
     ]
    }
   ],
   "source": [
    "for i in ['uber_division_w1','uber_super_dept_w1',\n",
    "                                                                                                                         'uber_dept_w1','uber_cat_w1',\n",
    "                                                                                                                         'uber_subcat_w1','seller_id']:\n",
    "    print(i,df[i].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796721"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['uber_subcat_w1'].isnull()==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3128108"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_ds = '2023-01-16'\n",
    "partitionDate = spark.sql(\"\"\"show partitions dmas.dfm\"\"\").rdd.flatMap(lambda x:x).map(lambda x : x.replace(\"ts=\",\"\")).max()\n",
    "\n",
    "\n",
    "\n",
    "query = spark.sql(\"\"\"SELECT dctr.*,\n",
    "                            coalesce(pcoc.catalog_item_id, catalog_item_id_dctr) as catalog_item_id,\n",
    "                            coalesce(pcoc.us_seller_id, seller_id_dctr) as seller_id,\n",
    "                            dmas.ad_item_brand brand_nm,\n",
    "                            darm.division_id AS division_id,\n",
    "                            darm.super_department_id AS super_department_id,\n",
    "                            darm.department_id AS department_id,\n",
    "                            darm.category_id AS category_id,\n",
    "                            darm.sub_category_id AS sub_category_id\n",
    "\n",
    "                    FROM (SELECT DISTINCT adid, source_id, is_mobile, seller_id as seller_id_dctr, split(adid,'_')[0] as catalog_item_id_dctr\n",
    "                          FROM sem.daily_criteria_traffic_revenue_v2_tmp\n",
    "                          WHERE \n",
    "                               customer_id_p = '1' \n",
    "                               AND (source_id = 2 or source_id=4)\n",
    "                               AND adtype_p ='pla' \n",
    "                               AND structure_version_p = 2\n",
    "                               AND revenue_source_p = 'fair15'\n",
    "                               AND date_string_p >= date_sub('__today__', 139) \n",
    "                               AND date_string_p <= date_sub('__today__', 7)) dctr\n",
    "\n",
    "                          LEFT JOIN sem.pla_catalog_offer_criteria_shopping pcoc\n",
    "                          ON dctr.adid = pcoc.adid\n",
    "\n",
    "\n",
    "                          LEFT JOIN(\n",
    "                                    SELECT adid, last(division_id) as division_id,\n",
    "                                    last(sub_category_id) as sub_category_id, last(super_department_id) as super_department_id,\n",
    "                                    last(department_id) as department_id, last(category_id) as category_id\n",
    "                                    FROM sem.daily_adid_rh_metrics_v2 \n",
    "                                    WHERE customer_id_p=1\n",
    "                                    AND structure_version_p = 2\n",
    "                                    AND revenue_source_p = 'fair15'\n",
    "                                    AND date_string >= date_sub('__today__', 139) \n",
    "                                    AND date_string <= date_sub('__today__', 7)\n",
    "                                    GROUP BY adid\n",
    "                                    ) darm\n",
    "                        ON dctr.adid = darm.adid \n",
    "                        \n",
    "                          LEFT JOIN (SELECT item_id as catlg_item_id,\n",
    "                                    max(lower(REGEXP_REPLACE(item_brand_name, '[^0-9A-Za-z]', ''))) as ad_item_brand\n",
    "                                    FROM dmas.dfm\n",
    "                                    WHERE ts=__partitionDate__\n",
    "                                    GROUP BY item_id) dmas\n",
    "                          ON coalesce(pcoc.catalog_item_id, dctr.catalog_item_id_dctr) = dmas.catlg_item_id\n",
    "                          \"\"\".replace('__today__', today_ds).replace('__partitionDate__', partitionDate))\n",
    "\n",
    "query.createOrReplaceTempView(\"dctr_pla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "days_backtrack_end = 6\n",
    "days_backtrack_start = 0\n",
    "bad_days = \"\"\"('2022-11-07', '2022-11-14', '2022-11-21', '2022-11-23', '2022-11-24', '2022-11-25')\"\"\"\n",
    "query = \"\"\"WITH             \n",
    "    dctr\n",
    "    AS\n",
    "    (SELECT dctr_v2.adid,\n",
    "            dctr_v2.source_id,\n",
    "            dctr_v2.is_mobile,\n",
    "            SUM(impressions) AS impressions,\n",
    "            SUM(clicks) AS clicks,\n",
    "            SUM(adspend) AS adspend,\n",
    "            SUM(qty) AS qty,\n",
    "            MAX(att_lineitems_brand) AS orders_brand,\n",
    "            MAX(att_lineitems_seller) AS orders_seller,\n",
    "            MAX(attributed_gmv_brand) AS gmv_revenue_adjustment_brand,\n",
    "            MAX(attributed_gmv_seller) AS gmv_revenue_adjustment_seller,\n",
    "            -- SUM(orders) AS orders,\n",
    "            -- SUM(gmv_revenue_adjustment) AS gmv_revenue_adjustment,\n",
    "            SUM(cost) AS cost,\n",
    "            SUM(estimated_cp) AS estimated_cp\n",
    "\n",
    "    FROM (SELECT * FROM sem.daily_criteria_traffic_revenue_v2_tmp\n",
    "           WHERE customer_id_p = '1' \n",
    "           AND (source_id = 2 or source_id=4)\n",
    "           AND adtype_p ='pla' \n",
    "           AND structure_version_p = 2\n",
    "           AND revenue_source_p = 'fair15'\n",
    "           AND date_string_p >= date_sub('__today__', __daysback_end__) \n",
    "           AND date_string_p <= date_sub('__today__', __daysback_start__) \n",
    "           AND date_string_p NOT IN __bad_days__) dctr_v2\n",
    "    LEFT JOIN (\n",
    "    SELECT adid, is_mobile, source_id, SUM(att_lineitems_brand) as att_lineitems_brand, SUM(att_lineitems_seller) as att_lineitems_seller,\n",
    "            SUM(attributed_gmv_brand) as attributed_gmv_brand, SUM(attributed_gmv_seller) as attributed_gmv_seller\n",
    "    FROM fair_dev.pt3_14day_report_nas_test \n",
    "    WHERE\n",
    "            ad_date >= date_sub('__today__', __daysback_end__) \n",
    "            AND ad_date <= date_sub('__today__', __daysback_start__) \n",
    "            AND ad_date NOT IN __bad_days__ \n",
    "            AND adid != 'NULL'\n",
    "            AND is_pla = 1\n",
    "    GROUP BY adid, is_mobile, source_id\n",
    "    ) coop_14d\n",
    "    ON dctr_v2.adid = coop_14d.adid\n",
    "    AND dctr_v2.is_mobile = coop_14d.is_mobile\n",
    "    AND dctr_v2.source_id = coop_14d.source_id\n",
    "\n",
    "    GROUP BY dctr_v2.adid, dctr_v2.is_mobile, dctr_v2.source_id)\n",
    "\n",
    "       --final query\n",
    "       SELECT  dctr_pla.catalog_item_id AS catalog_item_id,\n",
    "               dctr_pla.seller_id AS seller_id,\n",
    "               dctr_pla.source_id AS source_id,\n",
    "               dctr_pla.adid AS adid,\n",
    "               dctr_pla.brand_nm AS brand_nm,\n",
    "               dctr_pla.division_id AS division_id,\n",
    "               dctr_pla.super_department_id AS super_department_id,\n",
    "               dctr_pla.department_id AS department_id,\n",
    "               dctr_pla.category_id AS category_id,\n",
    "               dctr_pla.sub_category_id AS sub_category_id,\n",
    "               dctr_pla.is_mobile AS is_mobile,\n",
    "               dctr.impressions/__tot_days__ AS impressions,\n",
    "               dctr.clicks/__tot_days__ AS clicks,\n",
    "               dctr.adspend/__tot_days__ AS adspend,\n",
    "               dctr.qty/__tot_days__ AS qty,\n",
    "               dctr.orders_brand/__tot_days__ AS orders_brand,\n",
    "               dctr.orders_seller/__tot_days__ AS orders_seller,\n",
    "               dctr.gmv_revenue_adjustment_seller/__tot_days__ AS gmv_revenue_adjustment_seller,\n",
    "               dctr.gmv_revenue_adjustment_brand/__tot_days__ AS gmv_revenue_adjustment_brand,\n",
    "               dctr.cost/__tot_days__ AS cost,\n",
    "               (dctr.gmv_revenue_adjustment_seller - dctr.cost)/__tot_days__ AS margin_seller,\n",
    "               (dctr.gmv_revenue_adjustment_brand - dctr.cost)/__tot_days__ AS margin_brand,\n",
    "               dctr.estimated_cp/__tot_days__ AS estimated_cp,\n",
    "               dctr.clicks/dctr.impressions AS ctr,\n",
    "               dctr.adspend/dctr.clicks AS cpc,\n",
    "               dctr.gmv_revenue_adjustment_brand/dctr.clicks AS rpc_brand,\n",
    "               dctr.gmv_revenue_adjustment_seller/dctr.clicks AS rpc_seller,\n",
    "               dctr.gmv_revenue_adjustment_brand/dctr.adspend AS roas_brand,\n",
    "               dctr.gmv_revenue_adjustment_seller/dctr.adspend AS roas_seller,\n",
    "               dctr.orders_brand/dctr.clicks AS convrt_brand,\n",
    "               dctr.orders_seller/dctr.clicks AS convrt_seller,\n",
    "               dctr.gmv_revenue_adjustment_brand/dctr.orders_brand AS ordersize_brand,\n",
    "               dctr.gmv_revenue_adjustment_seller/dctr.orders_seller AS ordersize_seller,\n",
    "               dctr.estimated_cp/dctr.orders_brand AS cpsize_brand,\n",
    "               dctr.estimated_cp/dctr.orders_seller AS cpsize_seller,\n",
    "               (dctr.gmv_revenue_adjustment_brand - dctr.cost)/dctr.clicks AS mpc_brand,\n",
    "               (dctr.gmv_revenue_adjustment_seller - dctr.cost)/dctr.clicks AS mpc_seller,\n",
    "               (dctr.gmv_revenue_adjustment_brand - dctr.cost)/dctr.orders_brand AS mpo_brand,\n",
    "               (dctr.gmv_revenue_adjustment_seller - dctr.cost)/dctr.orders_seller AS mpo_seller,\n",
    "               dctr.gmv_revenue_adjustment_brand/dctr.estimated_cp AS rpcp_brand,\n",
    "               dctr.gmv_revenue_adjustment_seller/dctr.estimated_cp AS rpcp_seller\n",
    "\n",
    "        FROM dctr_pla LEFT JOIN dctr\n",
    "             ON dctr_pla.adid = dctr.adid\n",
    "             AND dctr_pla.source_id = dctr.source_id\n",
    "             AND dctr_pla.is_mobile = dctr.is_mobile\n",
    "        \"\"\"\n",
    "\n",
    "query = query.replace('__today__', today_ds)\n",
    "query = query.replace('__daysback_end__', str(days_backtrack_end))\n",
    "query = query.replace('__daysback_start__', str(days_backtrack_start))\n",
    "query = query.replace('__bad_days__', bad_days)\n",
    "bad_days_list = [i.replace(\"('\",'').replace(\"')\",'') for i in bad_days.split(\"', '\")]\n",
    "exclude_days = len([datetime.strptime(dat, '%Y-%m-%d') for dat in bad_days_list if datetime.strptime(dat, '%Y-%m-%d').date()>=datetime.strptime(today_ds, '%Y-%m-%d').date()- \\\n",
    "             timedelta(days=days_backtrack_end) and datetime.strptime(dat, '%Y-%m-%d').date()<=datetime.strptime(today_ds, '%Y-%m-%d').date()- timedelta(days=days_backtrack_start)])\n",
    "query = query.replace('__tot_days__', str(days_backtrack_end - days_backtrack_start+1-exclude_days))\n",
    "\n",
    "kk = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5186948"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13484"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kk[(kk['rpc_brand']!=0)&(kk['rpc_brand'].isnull()==False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(partition='date_string=2022-03-14'),\n",
       " Row(partition='date_string=2022-11-04'),\n",
       " Row(partition='date_string=latest')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('show partitions casesci_sem.items_w_dups_mapping_sem_offer_bidding').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+-----------------+------------+-----------+\n",
      "|   item_id|        product_type|id_unique_item|id_unique_variant|  product_id|date_string|\n",
      "+----------+--------------------+--------------+-----------------+------------+-----------+\n",
      "| 206072312|bicycle_bottom_br...|    PT971U0878|   PT971U0878V000|0RCN4SGTQ8D8|     latest|\n",
      "| 301700425|art_stencils_and_...|  PT4095U14794| PT4095U14794V000|0RCNEX2NP734|     latest|\n",
      "| 939541820|  art_pads_and_paper|   PT3743U3259|  PT3743U3259V000|0RCNM2Q1J8BV|     latest|\n",
      "|1380922101| christmas_stockings|    PT973U2885|   PT973U2885V001|0RCO6DIEZQAL|     latest|\n",
      "| 632401383|    cell_phone_cases|  PT012U652213| PT012U652213V001|0RCO7I3GIA8C|     latest|\n",
      "|1280002434|            t_shirts|  PT001U236191| PT001U236191V003|0RCOGTIN8MNB|     latest|\n",
      "|1792852021|   disposable_gloves|   PT3509U0424|  PT3509U0424V001|0RCOIFC7ZNTG|     latest|\n",
      "|1466735574|    blouses_and_tops|  PT000U360019| PT000U360019V007|0RCOW2R1MG1M|     latest|\n",
      "| 104213440|        night_lights|  PT2088U17212| PT2088U17212V000|0RCP0QD44HSV|     latest|\n",
      "|1668322544|    blouses_and_tops| PT000U1235510|PT000U1235510V003|0RCP6603QRHK|     latest|\n",
      "| 995982645|               rings|   PT014U61673|  PT014U61673V001|0RCPE4T3DQSR|     latest|\n",
      "| 737850606|            t_shirts| PT001U1395875|PT001U1395875V002|0RCPG0PXRG0Q|     latest|\n",
      "|1879667519|      ib_fence_posts|   PT2822U0153|  PT2822U0153V002|0RCQ17J6OJHD|     latest|\n",
      "| 507757821|            t_shirts| PT001U2448698|PT001U2448698V001|0RCQ2VXOCPQ1|     latest|\n",
      "| 933522411|ib_artificial_flo...|  PT1286U28971| PT1286U28971V000|0RCQD75B21C7|     latest|\n",
      "|1650147059|       ib_hair_masks|   PT3174U0557|  PT3174U0557V001|0RCQKD1JI2T8|     latest|\n",
      "| 882800878|             sconces|   PT1578U0888|  PT1578U0888V000|0RCQRFYWR9PI|     latest|\n",
      "|1259244119|            sweaters|  PT006U129858| PT006U129858V002|0RCQUJY79TKL|     latest|\n",
      "|1340007079|sweatshirts_and_h...|  PT002U210094| PT002U210094V006|0RCR2BI0B4PA|     latest|\n",
      "| 766050539|            t_shirts| PT001U1330531|PT001U1330531V000|0RCRCTGMRKQW|     latest|\n",
      "+----------+--------------------+--------------+-----------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from casesci_sem.items_w_dups_mapping_sem_offer_bidding where date_string='latest'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454891131"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from casesci_sem.items_w_dups_mapping_sem_offer_bidding where date_string='latest'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454891131"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select distinct * from casesci_sem.items_w_dups_mapping_sem_offer_bidding where date_string='latest'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT item_id)|\n",
      "+-----------------------+\n",
      "|              454823627|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct item_id) from casesci_sem.items_w_dups_mapping_sem_offer_bidding where date_string='latest'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|count(DISTINCT id_unique_item)|\n",
      "+------------------------------+\n",
      "|                      61508686|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct id_unique_item) from casesci_sem.items_w_dups_mapping_sem_offer_bidding where date_string='latest'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|count(DISTINCT id_unique_variant)|\n",
      "+---------------------------------+\n",
      "|                        138688922|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct id_unique_variant) from casesci_sem.items_w_dups_mapping_sem_offer_bidding where date_string='latest'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    item_agg.write.mode(\"overwrite\").insertInto(output_hive_table, overwrite=True)\n",
    "    spark.catalog.clearCache()\n",
    "    spark._jsparkSession.sharedState().cacheManager().clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['site_total_page_views_w1',\n",
       " 'site_total_add_to_carts_w1',\n",
       " 'site_total_orders_w1',\n",
       " 'site_total_revenue_w1',\n",
       " 'site_atcpv_w1',\n",
       " 'site_convrt_w1',\n",
       " 'site_rpv_w1',\n",
       " 'site_mpv_w1',\n",
       " 'site_total_page_views_w2',\n",
       " 'site_total_add_to_carts_w2',\n",
       " 'site_total_orders_w2',\n",
       " 'site_total_revenue_w2',\n",
       " 'site_atcpv_w2',\n",
       " 'site_convrt_w2',\n",
       " 'site_rpv_w2',\n",
       " 'site_mpv_w2']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in df.columns if 'site' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ['uber_super_dept_w1','uber_division_w1','uber_dept_w1','uber_cat_w1','uber_subcat_w1']:\n",
    "#     df[i] = df[i].astype(str)\n",
    "for i in ['uber_curr_item_price_w1','uber_avg_overall_rating_w1']:\n",
    "    df[i] = df[i].apply(lambda x: pd.to_numeric(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.format(\"csv\").option(\"header\",\"true\").load('gs://msc_fair_airflow/rpc_model/sample_data_full_feat/df_feat_all_coop_item_2023-01-16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType,BooleanType,DateType, FloatType\n",
    "sdf=sdf.withColumn(\"uber_curr_item_price_w1\",sdf.uber_curr_item_price_w1.cast(FloatType()))\n",
    "sdf=sdf.withColumn(\"uber_avg_overall_rating_w1\",sdf.uber_avg_overall_rating_w1.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitionDate = spark.sql(\"\"\"show partitions dmas.dfm\"\"\").rdd.flatMap(lambda x:x).map(lambda x : x.replace(\"ts=\",\"\")).max()\n",
    "\n",
    "item_info_dfm = spark.sql(\"\"\"SELECT item_id as catlg_item_id,\n",
    "                                        max(lower(REGEXP_REPLACE(item_brand_name, '[^0-9A-Za-z]', ''))) as ad_item_brand\n",
    "                FROM dmas.dfm\n",
    "                WHERE ts='\"\"\"+partitionDate+\"\"\"'\n",
    "                GROUP BY item_id\"\"\")\n",
    "\n",
    "sdf = sdf.join(item_info_dfm,sdf.catalog_item_id == item_info_dfm.catlg_item_id,how='left').drop(item_info_dfm.catlg_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('weight_price_cart_w1', sdf.site_total_add_to_carts_w1*(sdf.uber_curr_item_price_w1-sdf.uber_price_change_value_w1))\n",
    "sdf = sdf.withColumn('weight_price_cart_w2', sdf.site_total_add_to_carts_w2*(sdf.uber_curr_item_price_w1-sdf.uber_price_change_value_w1))\n",
    "\n",
    "sdf = sdf.withColumn('weight_price_orders_w1', sdf.site_total_orders_w1*(sdf.uber_curr_item_price_w1-sdf.uber_price_change_value_w1))\n",
    "sdf = sdf.withColumn('weight_price_orders_w2', sdf.site_total_orders_w2*(sdf.uber_curr_item_price_w1-sdf.uber_price_change_value_w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('sdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT sdf.*, brand_group.brand_weight_price_cart_w1,brand_group.brand_weight_price_cart_w2,\n",
    "                            brand_group.brand_weight_price_orders_w1,brand_group.brand_weight_price_orders_w2,\n",
    "                            seller_group.seller_weight_price_cart_w1,seller_group.seller_weight_price_cart_w2,\n",
    "                            seller_group.seller_weight_price_orders_w1,seller_group.seller_weight_price_orders_w2\n",
    "            FROM sdf \n",
    "            LEFT JOIN (select ad_item_brand, sum(weight_price_cart_w1)/sum(site_total_add_to_carts_w1) as brand_weight_price_cart_w1,\n",
    "              sum(weight_price_cart_w2)/sum(site_total_add_to_carts_w2) as brand_weight_price_cart_w2,\n",
    "              sum(weight_price_orders_w1)/sum(site_total_orders_w1) as brand_weight_price_orders_w1,\n",
    "              sum(weight_price_orders_w2)/sum(site_total_orders_w2) as brand_weight_price_orders_w2\n",
    "              from sdf\n",
    "              group by ad_item_brand) brand_group\n",
    "            ON sdf.ad_item_brand = brand_group.ad_item_brand\n",
    "            LEFT JOIN (select seller_id, sum(weight_price_cart_w1)/sum(site_total_add_to_carts_w1) as seller_weight_price_cart_w1,\n",
    "              sum(weight_price_cart_w2)/sum(site_total_add_to_carts_w2) as seller_weight_price_cart_w2,\n",
    "              sum(weight_price_orders_w1)/sum(site_total_orders_w1) as seller_weight_price_orders_w1,\n",
    "              sum(weight_price_orders_w2)/sum(site_total_orders_w2) as seller_weight_price_orders_w2\n",
    "              from sdf\n",
    "              group by seller_id) seller_group\n",
    "            ON sdf.seller_id = seller_group.seller_id\"\"\").coalesce(1).write.option(\"header\", \"true\").csv('gs://msc_fair_airflow/rpc_model/sample_data_full_feat/df_feat_all_coop_item_2023-01-16_new.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (245,246,247,248,249,284) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('gs://msc_fair_airflow/rpc_model/sample_data_full_feat/df_feat_all_coop_item_2023-01-16_new.csv/part-00000-b2887bfb-6832-4641-a1dd-732184f65274-c000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('sem_clicks_w1',ascending=False)\n",
    "df = df.head(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "for i in [k for k in df.columns if '_l' in k]:\n",
    "    df[i] = df[i].fillna(0)\n",
    "df_eval = df.copy()\n",
    "# prepare train features\n",
    "features = config[\"features\"]\n",
    "# clean features (all should be numerics)\n",
    "features_set = ['uber_curr_item_price_w1', 'uber_avg_overall_rating_w1'] #hard coded for hot fix\n",
    "df = convert_numeric(df, features_set)\n",
    "\n",
    "# do train-val split\n",
    "df['fold'] = 0\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "for i, (_, val_idx) in enumerate(kf.split(df)):\n",
    "    df['fold'].iloc[val_idx] = i\n",
    "    \n",
    "# filters\n",
    "convrt_trn_brand_filter = (df[source_id_name]==2) & (df.sem_convrt_brand_l>=0) & (df.sem_convrt_brand_l<=0.3)\n",
    "convrt_val_brand_filter = (df[source_id_name]==2) & (df.sem_convrt_brand_l>=0) & (df.sem_convrt_brand_l<=0.3)\n",
    "#    ordersize_trn_filter = (df[source_id_name]==2) & (df.sem_orders_l>=3) & (df.sem_ordersize_l>=0) & (df.sem_ordersize_l<=500)\n",
    "ordersize_trn_brand_filter = (df[source_id_name]==2) & (df.sem_ordersize_brand_l>=0) & (df.sem_ordersize_brand_l<=500)\n",
    "ordersize_val_brand_filter = (df[source_id_name]==2) & (df.sem_ordersize_brand_l>=0) & (df.sem_ordersize_brand_l<=500)\n",
    "rpc_trn_brand_filter = (df[source_id_name]==2) & (df.sem_ordersize_brand_l>=0) & (df.sem_ordersize_brand_l<=500)\n",
    "rpc_val_brand_filter = (df[source_id_name]==2) & (df.sem_ordersize_brand_l>=0) & (df.sem_ordersize_brand_l<=500)\n",
    "\n",
    "convrt_trn_seller_filter = (df[source_id_name]==2) & (df.sem_convrt_seller_l>=0) & (df.sem_convrt_seller_l<=0.3)\n",
    "convrt_val_seller_filter = (df[source_id_name]==2) & (df.sem_convrt_seller_l>=0) & (df.sem_convrt_seller_l<=0.3)\n",
    "#    ordersize_trn_filter = (df[source_id_name]==2) & (df.sem_orders_l>=3) & (df.sem_ordersize_l>=0) & (df.sem_ordersize_l<=500)\n",
    "ordersize_trn_seller_filter = (df[source_id_name]==2) & (df.sem_ordersize_seller_l>=0) & (df.sem_ordersize_seller_l<=500)\n",
    "ordersize_val_seller_filter = (df[source_id_name]==2) & (df.sem_ordersize_seller_l>=0) & (df.sem_ordersize_seller_l<=500)\n",
    "rpc_trn_seller_filter = (df[source_id_name]==2) & (df.sem_ordersize_seller_l>=0) & (df.sem_ordersize_seller_l<=500)\n",
    "rpc_val_seller_filter = (df[source_id_name]==2) & (df.sem_ordersize_seller_l>=0) & (df.sem_ordersize_seller_l<=500)\n",
    "filter_dic = {'convrt':{'brand':[convrt_trn_brand_filter, convrt_val_brand_filter], 'seller':[convrt_trn_seller_filter,convrt_val_seller_filter]},\\\n",
    "              'ordersize':{'brand':[ordersize_trn_brand_filter, ordersize_val_brand_filter], 'seller':[ordersize_trn_seller_filter,ordersize_val_seller_filter]},\\\n",
    "              'rpc':{'brand':[rpc_trn_brand_filter, rpc_val_brand_filter], 'seller':[rpc_trn_seller_filter,rpc_val_seller_filter]}}\n",
    "\n",
    "df_eval['pred_convrt_brand'] = np.nan\n",
    "df_eval['pred_ordersize_brand'] = np.nan\n",
    "df_eval['pred_rpc_brand'] = np.nan\n",
    "df_eval['pred_convrt_seller'] = np.nan\n",
    "df_eval['pred_ordersize_seller'] = np.nan\n",
    "df_eval['pred_rpc_seller'] = np.nan\n",
    "\n",
    "targets = ['seller', 'brand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seller_weight_price_cart_w1',\n",
       " 'seller_weight_price_cart_w2',\n",
       " 'seller_weight_price_orders_w1',\n",
       " 'seller_weight_price_orders_w2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar='seller'\n",
    "[tar+'_'+i for i in ['weight_price_cart_w1','weight_price_cart_w2','weight_price_orders_w1','weight_price_orders_w2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y0c07y1/.local/lib/python3.6/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/home/y0c07y1/.local/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/y0c07y1/.local/lib/python3.6/site-packages/lightgbm/basic.py:1491: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via 'params' instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.01552\tvalid_1's rmse: 0.0267574\n",
      "[200]\ttraining's rmse: 0.0115921\tvalid_1's rmse: 0.0266837\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's rmse: 0.0188295\tvalid_1's rmse: 0.0265497\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 10.72\tvalid_1's rmse: 31.0617\n",
      "[200]\ttraining's rmse: 6.17085\tvalid_1's rmse: 32.3838\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's rmse: 14.382\tvalid_1's rmse: 29.4983\n",
      "model1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.0151784\tvalid_1's rmse: 0.0315396\n",
      "[200]\ttraining's rmse: 0.0114777\tvalid_1's rmse: 0.0313758\n",
      "[300]\ttraining's rmse: 0.00979235\tvalid_1's rmse: 0.0312835\n",
      "[400]\ttraining's rmse: 0.00834402\tvalid_1's rmse: 0.0313015\n",
      "[500]\ttraining's rmse: 0.00718451\tvalid_1's rmse: 0.0313026\n",
      "Early stopping, best iteration is:\n",
      "[376]\ttraining's rmse: 0.00866863\tvalid_1's rmse: 0.0312822\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 10.3238\tvalid_1's rmse: 32.9433\n",
      "[200]\ttraining's rmse: 5.4748\tvalid_1's rmse: 37.0458\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's rmse: 26.0121\tvalid_1's rmse: 28.4283\n",
      "model2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.0156549\tvalid_1's rmse: 0.0240376\n",
      "[200]\ttraining's rmse: 0.0115607\tvalid_1's rmse: 0.0241812\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's rmse: 0.0179009\tvalid_1's rmse: 0.0239554\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 9.66092\tvalid_1's rmse: 76.5816\n",
      "[200]\ttraining's rmse: 5.46269\tvalid_1's rmse: 89.787\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's rmse: 29.3723\tvalid_1's rmse: 25.632\n",
      "model3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.0157439\tvalid_1's rmse: 0.0313314\n",
      "[200]\ttraining's rmse: 0.0119536\tvalid_1's rmse: 0.032029\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's rmse: 0.0253177\tvalid_1's rmse: 0.0309705\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 9.85135\tvalid_1's rmse: 37.9134\n",
      "[200]\ttraining's rmse: 5.81706\tvalid_1's rmse: 42.9068\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's rmse: 25.1424\tvalid_1's rmse: 32.6561\n",
      "model4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.0154922\tvalid_1's rmse: 0.0330825\n",
      "[200]\ttraining's rmse: 0.0114792\tvalid_1's rmse: 0.0331726\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's rmse: 0.0161233\tvalid_1's rmse: 0.0329099\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 12.2139\tvalid_1's rmse: 51.256\n",
      "[200]\ttraining's rmse: 6.41527\tvalid_1's rmse: 59.0289\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's rmse: 27.4684\tvalid_1's rmse: 25.644\n",
      "model0\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.0124131\tvalid_1's rmse: 0.0160041\n",
      "[200]\ttraining's rmse: 0.0102528\tvalid_1's rmse: 0.0168225\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's rmse: 0.0139033\tvalid_1's rmse: 0.0158376\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 15.2463\tvalid_1's rmse: 50.2436\n",
      "[200]\ttraining's rmse: 5.76868\tvalid_1's rmse: 49.1016\n",
      "[300]\ttraining's rmse: 2.8322\tvalid_1's rmse: 49.051\n",
      "[400]\ttraining's rmse: 1.62108\tvalid_1's rmse: 49.0066\n",
      "[500]\ttraining's rmse: 0.970333\tvalid_1's rmse: 49.0025\n",
      "[600]\ttraining's rmse: 0.601007\tvalid_1's rmse: 49.0038\n",
      "[700]\ttraining's rmse: 0.391833\tvalid_1's rmse: 49.0247\n",
      "Early stopping, best iteration is:\n",
      "[511]\ttraining's rmse: 0.934396\tvalid_1's rmse: 48.9939\n",
      "model1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.0122601\tvalid_1's rmse: 0.0171695\n",
      "[200]\ttraining's rmse: 0.0101831\tvalid_1's rmse: 0.0173886\n",
      "Early stopping, best iteration is:\n",
      "[80]\ttraining's rmse: 0.0129035\tvalid_1's rmse: 0.0171622\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 15.2286\tvalid_1's rmse: 49.2204\n",
      "[200]\ttraining's rmse: 5.6407\tvalid_1's rmse: 47.177\n",
      "[300]\ttraining's rmse: 2.73727\tvalid_1's rmse: 47.1347\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's rmse: 6.34894\tvalid_1's rmse: 47.091\n",
      "model2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.0121209\tvalid_1's rmse: 0.0172157\n",
      "[200]\ttraining's rmse: 0.0100251\tvalid_1's rmse: 0.0173728\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's rmse: 0.0123486\tvalid_1's rmse: 0.0171972\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 16.0153\tvalid_1's rmse: 40.0087\n",
      "[200]\ttraining's rmse: 5.68573\tvalid_1's rmse: 39.0657\n",
      "[300]\ttraining's rmse: 2.81054\tvalid_1's rmse: 39.1325\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttraining's rmse: 6.93303\tvalid_1's rmse: 38.9325\n",
      "model3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.0119998\tvalid_1's rmse: 0.019427\n",
      "[200]\ttraining's rmse: 0.00996287\tvalid_1's rmse: 0.0199802\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's rmse: 0.0134681\tvalid_1's rmse: 0.0190743\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 15.3436\tvalid_1's rmse: 36.1316\n",
      "[200]\ttraining's rmse: 5.33047\tvalid_1's rmse: 35.2865\n",
      "[300]\ttraining's rmse: 2.72155\tvalid_1's rmse: 35.2487\n",
      "[400]\ttraining's rmse: 1.66246\tvalid_1's rmse: 35.3074\n",
      "[500]\ttraining's rmse: 1.0367\tvalid_1's rmse: 35.2903\n",
      "Early stopping, best iteration is:\n",
      "[326]\ttraining's rmse: 2.36373\tvalid_1's rmse: 35.2238\n",
      "model4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.0123631\tvalid_1's rmse: 0.0162082\n",
      "[200]\ttraining's rmse: 0.0101214\tvalid_1's rmse: 0.0162688\n",
      "[300]\ttraining's rmse: 0.00889199\tvalid_1's rmse: 0.0164295\n",
      "Early stopping, best iteration is:\n",
      "[116]\ttraining's rmse: 0.0119102\tvalid_1's rmse: 0.0162008\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 15.3822\tvalid_1's rmse: 46.7869\n",
      "[200]\ttraining's rmse: 5.7571\tvalid_1's rmse: 45.6841\n",
      "[300]\ttraining's rmse: 2.90278\tvalid_1's rmse: 45.4317\n",
      "[400]\ttraining's rmse: 1.65288\tvalid_1's rmse: 45.357\n",
      "[500]\ttraining's rmse: 1.06304\tvalid_1's rmse: 45.3769\n",
      "Early stopping, best iteration is:\n",
      "[368]\ttraining's rmse: 1.95756\tvalid_1's rmse: 45.341\n"
     ]
    }
   ],
   "source": [
    "for tar in targets:\n",
    "    for fold in range(5):\n",
    "        print('model'+str(fold))\n",
    "        # convrt model\n",
    "        model_features = features['convrt'][tar]+[tar+'_'+i for i in ['weight_price_cart_w1','weight_price_cart_w2','weight_price_orders_w1','weight_price_orders_w2']]\n",
    "        target_col = \"sem_convrt_\"+tar+\"_l\"\n",
    "        model_name = f'convrt_f{fold}'+'_'+tar\n",
    "        trn_mask = filter_dic['convrt'][tar][0] & (df.fold != fold)\n",
    "        df_trn = df[trn_mask]\n",
    "        val_mask = filter_dic['convrt'][tar][1] & (df.fold == fold)\n",
    "        df_val = df[val_mask]\n",
    "        train(df_trn, df_val, model_features, target_col, lgb_params, model_name, weight_col='sem_clicks_l')\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = eval(df_val, model_features, target_col, model_name, weight_col='sem_clicks_l')\n",
    "        df_eval['pred_convrt_'+tar].loc[val_mask] = y_pred\n",
    "\n",
    "        # order_size model\n",
    "        model_features = features['ordersize'][tar]+[tar+'_'+i for i in ['weight_price_cart_w1','weight_price_cart_w2','weight_price_orders_w1','weight_price_orders_w2']]\n",
    "        target_col = \"sem_ordersize_\"+tar+\"_l\"\n",
    "        model_name = f'ordersize_f{fold}'+'_'+tar\n",
    "        trn_mask = filter_dic['ordersize'][tar][0] & (df.fold != fold)\n",
    "        df_trn = df[trn_mask]\n",
    "        val_mask = filter_dic['ordersize'][tar][1] & (df.fold == fold)\n",
    "        df_val = df[val_mask]\n",
    "        train(df_trn, df_val, model_features, target_col, lgb_params, model_name, weight_col='sem_orders_'+tar+'_l')\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = eval(df_val, model_features, target_col, model_name, weight_col='sem_orders_'+tar+'_l')\n",
    "        df_eval['pred_ordersize_'+tar].loc[val_mask] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convrt oof based on lightgbm WRMSE:  0.02942\n",
      "convrt oof based on lightgbm WMAE:  0.01457\n",
      "ordersize oof based on lightgbm WRMSE:  28.13425\n",
      "ordersize oof based on lightgbm WMAE:  14.04540\n",
      "rpc oof based on lightgbm WRMSE:  0.94024\n",
      "rpc oof based on lightgbm WMAE:  0.29411\n",
      "convrt oof based on lightgbm WRMSE:  0.01702\n",
      "convrt oof based on lightgbm WMAE:  0.00640\n",
      "ordersize oof based on lightgbm WRMSE:  43.64110\n",
      "ordersize oof based on lightgbm WMAE:  23.82708\n",
      "rpc oof based on lightgbm WRMSE:  1.68347\n",
      "rpc oof based on lightgbm WMAE:  0.49071\n"
     ]
    }
   ],
   "source": [
    "for tar in targets:\n",
    "    df_eval['pred_rpc_'+tar] = df_eval['pred_convrt_'+tar] * df_eval['pred_ordersize_'+tar]\n",
    "    target_cols = ['sem_convrt_'+tar+'_l', 'sem_ordersize_'+tar+'_l', 'sem_rpc_'+tar+'_l']\n",
    "    pred_cols = ['pred_convrt_'+tar, 'pred_ordersize_'+tar, 'pred_rpc_'+tar]\n",
    "    weight_cols = ['sem_clicks_l', 'sem_orders_'+tar+'_l', 'sem_clicks_l']\n",
    "    for target_col, pred_col, weight_col in zip(target_cols, pred_cols, weight_cols):\n",
    "        filter_NoNA = df_eval[target_col].notnull() & df_eval[pred_col].notnull() & df_eval[weight_col].notnull()\n",
    "        WRMSE = wrmse(df_eval[target_col].loc[filter_NoNA].values, df_eval[pred_col].loc[filter_NoNA].values, df_eval[weight_col].loc[filter_NoNA].values)\n",
    "        WMAE = wmae(df_eval[target_col].loc[filter_NoNA].values, df_eval[pred_col].loc[filter_NoNA].values, df_eval[weight_col].loc[filter_NoNA].values)\n",
    "        print(f\"{'_'.join(pred_col.split('_')[1:-1])} oof based on lightgbm WRMSE: {WRMSE: .5f}\")\n",
    "        print(f\"{'_'.join(pred_col.split('_')[1:-1])} oof based on lightgbm WMAE: {WMAE: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr_torch.models import DIEN\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from deepctr_torch.inputs import SparseFeat, get_feature_names\n",
    "from deepctr_torch.models import DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('gs://msc_fair_airflow/rpc_model/sample_data_full_feat/df_feat_all_coop_item_2023-01-16.csv',nrows = 10000)\n",
    "sparse_features = ['uber_dept_w1',\n",
    " 'uber_subcat_w1',\n",
    " 'uber_super_dept_w1',\n",
    " 'uber_division_w1',\n",
    " 'uber_cat_w1']\n",
    "target = ['sem_rpc_seller_l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_json('gs://msc_fair_airflow/rpc_model/config_rpc.json')\n",
    "adid_name = config[\"hash_id\"][\"adid\"]\n",
    "is_mobile_name = config[\"hash_id\"][\"is_mobile\"]\n",
    "source_id_name = config[\"hash_id\"][\"source_id\"]\n",
    "features = config['features']\n",
    "model_features = features['convrt']['seller']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "# 2.count #unique features for each sparse field\n",
    "fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                          for feat in sparse_features]\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uber_dept_w1',\n",
       " 'uber_subcat_w1',\n",
       " 'catalog_item_id',\n",
       " 'source_id',\n",
       " 'seller_id',\n",
       " 'adid',\n",
       " 'uber_super_dept_w1',\n",
       " 'uber_division_w1',\n",
       " 'uber_cat_w1']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in list(set(data.columns)-set(model_features)) if '_brand_' not in i and '_l' not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./movielens_sample.txt\")\n",
    "sparse_features = [\"movie_id\", \"user_id\",\n",
    "                   \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = ['rating']\n",
    "\n",
    "# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "# 2.count #unique features for each sparse field\n",
    "fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                          for feat in sparse_features]\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "# 3.generate input data for model\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_model_input = {name: train[name] for name in feature_names}\n",
    "test_model_input = {name: test[name] for name in feature_names}\n",
    "# 4.Define Model,train,predict and evaluate\n",
    "\n",
    "device = 'cpu'\n",
    "use_cuda = True\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    print('cuda ready...')\n",
    "    device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 128 samples, validate on 32 samples, 1 steps per epoch\n",
      "Epoch 1/10\n",
      "0s - loss:  3.5633\n",
      "Epoch 2/10\n",
      "0s - loss:  3.5127\n",
      "Epoch 3/10\n",
      "0s - loss:  3.4692\n",
      "Epoch 4/10\n",
      "0s - loss:  3.4275\n",
      "Epoch 5/10\n",
      "0s - loss:  3.3875\n",
      "Epoch 6/10\n",
      "0s - loss:  3.3485\n",
      "Epoch 7/10\n",
      "0s - loss:  3.3075\n",
      "Epoch 8/10\n",
      "0s - loss:  3.2645\n",
      "Epoch 9/10\n",
      "0s - loss:  3.2192\n",
      "Epoch 10/10\n",
      "0s - loss:  3.1717\n"
     ]
    }
   ],
   "source": [
    "model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression', device=device)\n",
    "model.compile(\"adam\", \"mae\", metrics=['htdhtr'], )\n",
    "\n",
    "history = model.fit(train_model_input, train[target].values, batch_size=256, epochs=10, verbose=2,\n",
    "                    validation_split=0.2)\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>76</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>974658834</td>\n",
       "      <td>20,000 Leagues Under the Sea (1954)</td>\n",
       "      <td>Adventure|Children's|Fantasy|Sci-Fi</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>974697744</td>\n",
       "      <td>Dead Man Walking (1995)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>173</td>\n",
       "      <td>5</td>\n",
       "      <td>976316283</td>\n",
       "      <td>High Fidelity (2000)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>73</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>974677761</td>\n",
       "      <td>On the Waterfront (1954)</td>\n",
       "      <td>Crime|Drama</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>65</td>\n",
       "      <td>122</td>\n",
       "      <td>2</td>\n",
       "      <td>974699637</td>\n",
       "      <td>Player, The (1992)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>175</td>\n",
       "      <td>137</td>\n",
       "      <td>3</td>\n",
       "      <td>960328279</td>\n",
       "      <td>Rocky Horror Picture Show, The (1975)</td>\n",
       "      <td>Comedy|Horror|Musical|Sci-Fi</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>67</td>\n",
       "      <td>186</td>\n",
       "      <td>5</td>\n",
       "      <td>974753321</td>\n",
       "      <td>Meet the Parents (2000)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>143</td>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>965094644</td>\n",
       "      <td>Last Emperor, The (1987)</td>\n",
       "      <td>Drama|War</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>171</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>961055887</td>\n",
       "      <td>Star Trek: The Wrath of Khan (1982)</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>975648062</td>\n",
       "      <td>Lost in Space (1998)</td>\n",
       "      <td>Action|Sci-Fi|Thriller</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  movie_id  rating  timestamp  \\\n",
       "92        76        49       3  974658834   \n",
       "184       71         3       5  974697744   \n",
       "11         7       173       5  976316283   \n",
       "27        73       100       4  974677761   \n",
       "30        65       122       2  974699637   \n",
       "..       ...       ...     ...        ...   \n",
       "22       175       137       3  960328279   \n",
       "187       67       186       5  974753321   \n",
       "42       143       101       4  965094644   \n",
       "141      171        80       4  961055887   \n",
       "77        70        95       2  975648062   \n",
       "\n",
       "                                     title  \\\n",
       "92     20,000 Leagues Under the Sea (1954)   \n",
       "184                Dead Man Walking (1995)   \n",
       "11                    High Fidelity (2000)   \n",
       "27                On the Waterfront (1954)   \n",
       "30                      Player, The (1992)   \n",
       "..                                     ...   \n",
       "22   Rocky Horror Picture Show, The (1975)   \n",
       "187                Meet the Parents (2000)   \n",
       "42                Last Emperor, The (1987)   \n",
       "141    Star Trek: The Wrath of Khan (1982)   \n",
       "77                    Lost in Space (1998)   \n",
       "\n",
       "                                  genres  gender  age  occupation  zip  \n",
       "92   Adventure|Children's|Fantasy|Sci-Fi       1    5          15   15  \n",
       "184                                Drama       1    6          12  163  \n",
       "11                                Comedy       1    2           2  140  \n",
       "27                           Crime|Drama       0    6          12    9  \n",
       "30                          Comedy|Drama       1    1           0  162  \n",
       "..                                   ...     ...  ...         ...  ...  \n",
       "22          Comedy|Horror|Musical|Sci-Fi       1    4           7    3  \n",
       "187                               Comedy       1    2           7  151  \n",
       "42                             Drama|War       1    5           0    1  \n",
       "141              Action|Adventure|Sci-Fi       1    2          11  161  \n",
       "77                Action|Sci-Fi|Thriller       1    4          19  148  \n",
       "\n",
       "[160 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./movielens_sample.txt\")['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
